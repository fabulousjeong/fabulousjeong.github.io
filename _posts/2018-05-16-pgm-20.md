---
title: 20 Conditional Random Fields
category: ProbabilisticGraphicalModels
excerpt: |
  Conditional random field(CRF)는 마르코프 네트워크에서 파생된 주요 기법 중 하나입니다. 많은 분야에서 사용되며, 형태는 마르코프 네트워크와 비슷하지만 사용 목적은 약간 다릅니다. 


feature_text: |
  ## [Coursera] Probabilistic Graphical Models
  Daphne Koller의 Probabilistic Graphical Models 강의 정리

  ref: [https://www.coursera.org/learn/probabilistic-graphical-models/home](https://www.coursera.org/learn/probabilistic-graphical-models/home "coursera")

feature_image: "https://picsum.photos/2560/600/?image=798"
image: "https://picsum.photos/2560/600/?image=798"
comment: true
---


##### Conditional Random Fields

![](http://cfile30.uf.tistory.com/image/255CF93E58B0BE10191297 "Conditional Random Fields" "width:600px;float:center;padding-left:10px;")

Conditional random field(CRF)는 마르코프 네트워크에서 파생된 주요 기법 중 하나입니다. 많은 분야에서 사용되며, 형태는 마르코프 네트워크와 비슷하지만 사용 목적은 약간 다릅니다. 지금부터 우리는 입력 변수 X로 부터 출력변수 Y를 구하는 Task-예측을 하려고 합니다. 가령, 이미지 세분화에서는 입력값으로 픽셀의 값(RGB, 히스토그램)을 사용하고, 출력으로는 각 픽셀의 클래스(잔디, 하늘, 소, 물) 등이 될 수 있습니다. 텍스트 처리에서는 입력으로 각 문자의 단어를 사용하고, 이를 통해 그 단어의 클래스(사람, 장소, 조직, 등)를 예측 합니다. 이러한 것을 기존의 방법을 사용해서 수행 할 수는 없을까요? 왜 새로운 Conditional random field라는 것을 도입해야하는지 아래 예를 통해 살펴 보겠습니다.      


![](http://cfile25.uf.tistory.com/image/22325B3E58B0BE112A8805 "Correlation" "width:600px;float:center;padding-left:10px;")

기존 방법의 문제를 먼저 보겠습니다. 특정 슈퍼픽셀의 레이블 $C_i$를 예측하려 합니다. 이를 위해 슈퍼픽셀의 피쳐(RGB, 텍스쳐 히스토그램 등)를 처리하여, 슈퍼픽셀의 특성의 정의 할 수 있습니다. 여기서 레이블 $C_i$는 Y에 해당하며 슈퍼픽셀의 피쳐는 X에 해당합니다. 여기서 문제는 이러한 피쳐들이 서로 매우 높은 관련성(Correlation)을 가지고 있는 점입니다. 가령 텍스쳐 히스토그램은 슈펴픽셀 내 선의 방향을 표현합니다. 이는 슈퍼픽셀의 내부 구조나 텍스쳐와 매우 깊은 관련성을 가집니다. 이러한 경우 나이브 베이즈(Naive Bayes)을 사용하면, 피쳐들은 서로 독립적으로 표현되므로, 그들 간의 관련성은 무시됩니다. 따라서 실제 이러한 피쳐들은 독립이 아닌데, 이를 무시하고 독립적으로 간주했기 때문에 유사한 피쳐가 여러번 계산 됩니다. 당연히 좋치 않은 결과가 나올 것입니다. 그렇다면 이를 해결하기 위해 서로의 관련성을 표현하는 간선들로 피쳐들을 연결해야 할까요? 하지만 이것은 굉장히 어려운 일입니다. 피쳐들간의 관련성을 모델링해야하며, 간선들이 추가되어 전체 네트워크는 굉장히 복잡해질 것입니다.        

 이제 다른 방법으로 접근해보겠습니다. 이제 이미지의 피쳐에 대해서는 생각하지 않겠습니다. 이제 각 픽셀내 피쳐의 확률 분포에 대해서는 신경 쓰지 않습니다. 즉 갈색 픽셀 옆에 녹색 픽셀이 있을 확률 같은 것은 신경 쓰지 않습니다. 그저 주어진 X라는 특성을 통해 Y의 확률 분포를 모델링합니다. 앞의 모델에서는 주어진 전체 네트워크에 대한 확률 분포 $P(X, Y)$를 구하려 했다면, 위 방법은 주어진 X로 부터 Y의 분포 $P(Y\|X)$를 구합니다. 이를 Conditional random field(CRF)라 합니다. X가 조건으로 들어갔으므로 이제 더 이상 그들 사이의 관련성에 대해 신경 쓰지 않아도 됩니다.      
 
![](http://cfile26.uf.tistory.com/image/255FE33E58B0BE1223EBBE "Conditional random field" "width:600px;float:center;padding-left:10px;")

Conditional random field를 어떻게 구하는지 식으로 알아보겠습니다. 우선 Conditional random field 역시 네트워크의 특성을 나타내는 factor를 가지고 있습니다. Gibbs distribution때와 동일하게 그들을 곱해 전체 네트워크에 대한 unnormalized 확률 분포를 구합니다. X를 조건으로 두기위해, 위 식처럼 Y변수에 대한 합을 구합니다. 이제 X의 partition function $Z_{\Phi}(X)$는 X에 대한 확률 분포를 나타냅니다. 이를 앞에서 구한  전체 네트워크의 확률 분포에 나눠주면 Conditional random field를 구할 수 있습니다. 위 표는 이진 변수 x, y으로 이루어진 네트워크의 확률 분포를 나타냅니다. x0y0, x0y1를 더해 Z(x0)구한 다음 이를 x0y0, x0y1에 나눠 주면 x0에 대한 조건부 확률이 됩니다. x1에 대해서도 동일하게 적용 할 수 있습니다.  

![](http://cfile23.uf.tistory.com/image/2555A73E58B0BE132C6BAB "Conditional random field" "width:600px;float:center;padding-left:10px;")

이제, x, y가 이진 값을 가지고 factor가 위와 같이 지수함수로 표현 되는 logistic model의 경우를 보겠습니다. $\phi_i(X_i, Y=1)$인 경우 위 식에서 $1{X_i=1, Y=1}$은 X가 0이면 0, 1이면 1의 값을 가집니다. 따라서 X값을 그대로 가진다고 볼 수 있습니다. 즉 $\phi_i(X_i, Y=1)$=$exp{(w_i X_i)}$ 입니다.  $\phi_i(X_i, Y=0)$인 경우 X값에 상관없이 0이 곱해지므로 $\phi_i(X_i, Y=1)=1$입니다.  $P(Y=1, X_1, ..., X-n)$는 $\phi_i(X_i, Y=1)$의 곱과 같고 이는 지수 승에서 합으로 표현됩니다. $P(Y=0, X_1, ..., X-n)$는 1을 곱하는 것이므로 그 값은 1이 됩니다. partition function Z(X)는 Y에 대한 합으로 표현 할 수 있으므로 $1+exp(\Sigma w_i X_i)$와 같습니다. 이제 이를  $P(Y=1, X_1, ..., X-n)$에 나누면 주어진 X피쳐로 부터 Y가 1일 확률을 구할 수 있습니다. 이를 식으로 나타나면 위와 같습니다. 이 식은 오른쪽 그림처럼 시그모이드(sigmoid)함수와 형태인 것을 볼 수 있습니다. 따라서 logistic model은 CRF를 사용하여 매우 간단한 시그모이드 함수로 모델링 할 수 있습니다. 그리고 새로운 $X_i$가 추가 된다고 하더라도 위 식이 시그모이드 함수인 것은 변함 없습니다. 따라서 CRF에서는 입력 변수 사이의 관련성에 신경을 쓰지 않아도 된다는 것을 알 수 있습니다. 즉 피쳐를 늘려 모델의 변수가 증가한다고해도 각 입력 변수사이의 관련성에 대해 걱정하지 않아도 됩니다. 

##### Examples

![](http://cfile1.uf.tistory.com/image/236E943E58B0BE1423912B "Segmentation" "width:600px;float:center;padding-left:10px;")

앞선 이미지 세그멘테이션 예제를 통해 CRF의 개념을 보겠습니다. 각 Factor는 입력변수(히스토그램, 텍스쳐피쳐)는 레이블과 관련성을 가지고 있고 이는 factor로 표현됩니다. 그리고 이는 "소의 눈"과 같이 서로 다른 패치들로 나눠집니다. 그리고 이러한 패치들 역시 서로 관련성이 있습니다. 하지만 여기서는 신경쓰지 않을 것입니다. 우리는 여기서 각 패치의 입력 변수와 레이블 간의 factor를 통해 CRF를 구한 다음, 이것을 사용 할 것입니다. 그런 다음 성능을 높이기 위해 support vector machine,  boosting과 같은 분류기를 통해 학습시킵니다.   

![](http://cfile23.uf.tistory.com/image/2647C03E58B0BE1521B6EA "Language" "width:600px;float:center;padding-left:10px;")

![](http://cfile29.uf.tistory.com/image/2577493E58B0BE161AADB1 "Language" "width:600px;float:center;padding-left:10px;")

단어 레이블링의 경우도 비슷한 방법으로 수행됩니다. 각 단어와 레이블 사이의 관련성에 대한 factor가 있을 것입니다. 가령 대문자로 시작하면 사람이름일 가능성이 높다거나, 어미의 형태등에 따른 품사를 구분 할 수도 있습니다. 이때 역시 이러한 factor를 통해 CRF를 구한 다음, 각 단어가 어떤 품사를 가질지에 대한 확률 분포를 계산 할 수 있습니다.  

![](http://cfile22.uf.tistory.com/image/214DAF4458B0BE1716181F "Summary" "width:600px;float:center;padding-left:10px;")

CRF역시 Gibbs distribution과 같이 노말라이즈 과정이 있지만, 차이점은 한 변수에 대한 partition function으로 나누는 것입니다. 따라서 주어진 X에 대한 Y의 조건부 확률 분포를 표현 할 수 있었습니다. 우리가 구하고자하는 변수 외에 다른 변수들은 모두 조건으로 들어 갔기 때문에 이제 더 이상 그들 사이의 관련성에 대해 신경 쓸 필요가 없습니다. 따라서 CRF의 유용성은 서로 다른 변수 사이의 독립성에 대한 걱정없이 보다 실질적인 목표에 대한 예측 모델을 디자인 할 수 있다는 것입니다.










