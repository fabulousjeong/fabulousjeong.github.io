---
title: 12 Temporal Models HMMs(Hidden Markov Models)
category: ProbabilisticGraphicalModels
excerpt: |
  히든 마르코프 모델(Hidden Markov Model)은 probabilistic temporal models의 간단한 한 예입니다. 이 모델은 이전에 다룬 동적 베이지안 네트워크(Dynamic Bayesian Networks)의 한 예라고 볼 수도 있습니다.


feature_text: |
  ## [Coursera] Probabilistic Graphical Models
  Daphne Koller의 Probabilistic Graphical Models 강의 정리

  ref: https://www.coursera.org/learn/probabilistic-graphical-models/home

feature_image: "https://picsum.photos/2560/600/?image=798"
image: "https://picsum.photos/2560/600/?image=798"
comment: true
---


##### Hidden Markov Model

![](http://cfile26.uf.tistory.com/image/262D235058A1A50436EFCD "HMM" "width:600px;height:300px;float:center;padding-left:10px;")

히든 마르코프 모델(Hidden Markov Model)은 probabilistic temporal models의 간단한 한 예입니다. 이 모델은 이전에 다룬 동적 베이지안 네트워크(Dynamic Bayesian Networks)의 한 예라고 볼 수도 있습니다. 이제 다양한 응용 예제에 사용 할 수 있는 구조를 살펴 보겠습니다. 가장 간단한 마르코프 모델은 상태(State) 노드 S와 관찰(Observation)노드 O로 이루어져 있습니다. 상태 노드는 다음 단계의 상태노드와 연결 되어 있으며, 이는 현재 상태에서 사건이 발생할 확률을 보는 관찰 노드와도 연결되어 있습니다. 이제 이 2TBN 모델을 오른쪽과 같이 펼칠수 있습니다. 이 네트워크는 동일한 구조의 반복으로 이루어져있습니다. 이 모델은 상태의 전이, 각 상태에서의 관찰을 표현합니다. 히든 마르코프 모델은 상태의 전이를 잘 표현하는 내부구조를 가지고 있습니다. 아랫쪽에 위치한 그래프는 상태의 전이에 관한 CPD를 표현합니다. 여기에서 상태의 전이에 대한 구조를 볼 수 있습니다. $S_1$을 예로 들면 $S_2$로 전이될 확률이 0.7, 자신의 상태를 유지 할 확률이 0.3입니다. 따라서 전이 될 가능성이 더 높습니다. 그리고 $S_1$에서 이동할 확률은 $S_1$에 대한 CPD이므로 합이 1이 되어야 합니다.

![](http://cfile2.uf.tistory.com/image/223DC95058A1A5050FD580 "HMM" "width:600px;height:250px;float:center;padding-left:10px;")

2TBN 모델을 응용하여 실제 사례에 적용하는 방법은 매우 간단합니다. 따라서 이러한 구조는 광범위한 분야에 유용합니다. 로봇 위치 추적, 음성인식,  생물학적 열 분석, 문서 주석달기 등에 사용되었습니다. 특히 음성인식에는 지금도 HMM이 많이 사용됩니다.   

![](http://cfile22.uf.tistory.com/image/247FC65058A1A5061571BB "Robot" "width:600px;height:300px;float:center;padding-left:10px;")

위의 예는 HMM이 어떻게 로봇 위치 추적 모델을 표현하는지 보여줍니다. 여기는 몇가지 추가 변수가 있으므로 앞에서 보여준 간단한 HMM과는 조금 다르게 보입니다. 여기서 상태 변수 S는 현재 로봇의 위치를 나타냅니다. 또한 로봇이 어떻게 움직일지에 대한 외부제어신호 변수 u가 있습니다. 이 두 변수에 의해 다음 로봇의 위치 S'이 결정됩니다. 그리고 S와 map에 기반한 관측변수 O가 있습니다. 따라서 로봇의 관측 O는 현재 위치와 맵에 좌우 됩니다. Map은 변하지 않으므로 하나의 노드로만 표현되었습니다. 이 HMM모델은 기본 S-O로 구성된 구조에 약간 더 정확성을 주기위해 다른 변수를 추가하여 만들어 졌습니다. 로봇의 상태가 연속적으로 촘촘하게 변하지 않기 때문에 이 전이 모델은 성긴(sparsity) 구조를 가지고 있습니다.     

![](http://cfile1.uf.tistory.com/image/273BEA5058A1A50629B4E1 "Speech" "width:600px;height:300px;float:center;padding-left:10px;")

앞에서도 설명 했듯이 음성인식은 HMM이 가장 성공적으로 사용 되고 있는 분야입니다. 음성 인식에서의 문제는 노이즈가 많이있는 입력 문장(Dorothy lived in)이 들어 왔을 때, 이에 맞는 여러 출력 문장 중 하나를 예측하는데 있습니다.

![](http://cfile30.uf.tistory.com/image/26216F5058A1A50711D31C "Speech" "width:600px;height:300px;float:center;padding-left:10px;")

그렇다면 어떠한 방법으로 동작하는지 알아 보겠습니다. 가장 위에는 음성 신호의 주파수를 보여줍니다. 시간 축에대해 주파수를 가지고 있으며 이를 퓨리에 변환하여 아래와 같이 주파수 스펙트럼으로 나타 낼 수 있습니다. 그리고 이러한 연속적인 신호를 쪼갠 다음 각 신호에 대응하는 음소(phonemes)를 찾습니다. 그리고 그 음소의 집합과 단어를 매칭시킵니다.

![](http://cfile8.uf.tistory.com/image/230DD25058A1A5082DC9A5 "Phonetic" "width:600px;height:300px;float:center;padding-left:10px;")

위는 음소 알파벳입니다. 각 단어가 어떻게 음소로 쪼개지는지를 표현하고 있습니다. 가령 hurt는 HH ER T 음소들로 쪼개 질 수 있습니다.

![](http://cfile10.uf.tistory.com/image/21204E5058A1A5092B81E1 "Word HMM" "width:600px;height:300px;float:center;padding-left:10px;")

nine라는 단어는 위와 같이 n, ay, n 음소로 쪼개집니다. 위 구조는 HMM이며 각 시점에 대해 우리가 n 상태에 있는지, ay 상태에 있는지를 표현합니다. 여러 시점에 걸쳐 한 상태에 머무를 수도 있기 때문에 본인 상태로 돌아오는 엣지도 있습니다.

![](http://cfile22.uf.tistory.com/image/273F6D4A58A1A50A24DF39 "Phone HMM" "width:600px;height:300px;float:center;padding-left:10px;")

음소도 특정 시간동안 지속되므로, 한 음소 내에서도 위와 같이 처음, 중간, 끝 등 여러 상태(state)로 나눌수 있습니다. 위와 같은 표현법은 음소를 모델링하는데 일반적으로 사용됩니다. 위 그림은 한 음소에 대해서만 모델링 했습니다. 이제 이를 확장시켜 보겠습니다.

![](http://cfile30.uf.tistory.com/image/2647644A58A1A50A348B2C "Recognition HMM" "width:600px;height:300px;float:center;padding-left:10px;")

위 그래프는 조금 더 일반적인 상황을 모델링했습니다. 위 모델에는 시작 상태가 있으며 들어오는 신호를 보고 다음 노드로 전이 될 가능성을 추정합니다. 한 음소를 말할 때마다 이를 반복합니다. 최종적으로 이 신호가 어떤 수를 말한건지에 대한 확률을 얻어 냅니다. 그 다음 다시 시작 상태로 가서 지속적으로 음성인식을 할 수 있게 대기합니다. 따라서 위 모델은 각 노드를 이동할 확률부터 시작해서, 이 신호가 어떤 단어를 표현하는 것인지에 대한 확률을 얻어내므로, 확률론적 모델(probabilistic model)이라 할 수 있습니다.

![](http://cfile23.uf.tistory.com/image/2123044A58A1A50B064264 "Summary" "width:600px;height:300px;float:center;padding-left:10px;")

요약하자면, HMM은 단순히 DBN의 하위 클래스로 볼 수 있습니다. 마치 특정 구조를 띄고 있지 않는 것 처럼 보이더라도 그 속에는 상태의 전이 부분에서 sparsity나 변수의 반복 등 전형적 구조를 보입니다. 그리고 위의 예처럼 다양한 분야의 모델링에 응용할 수 있습니다.   
