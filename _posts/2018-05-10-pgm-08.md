---
title: 08 Naive Bayes
category: ProbabilisticGraphicalModels
excerpt: |
  지난 시간에는 베이지안 네트워크에서 두 노드의 값이 변할때 어떻게 서로 영향을 주는지 직관적인 추론(Reasoning)을 통해 알아봤습니다. 이번 시간에는 어떤 경우에 노드 사이에 서로 영향을 주는지를 보다 정확하게 살펴 보겠습니다. 일단 일반적인 경우의 예부터 보겠습니다.   


feature_text: |
  ## [Coursera] Probabilistic Graphical Models
  Daphne Koller의 Probabilistic Graphical Models 강의 정리

  ref: https://www.coursera.org/learn/probabilistic-graphical-models/home

feature_image: "https://picsum.photos/2560/600/?image=798"
image: "https://picsum.photos/2560/600/?image=798"
comment: true
---


##### Naive Bayes

![](http://cfile5.uf.tistory.com/image/2633724658951D5805EDBE "Naive Bayes" "width:600px;height:250px;float:center;padding-left:10px;")

베이지안 네트워크(Bayesian Networks) 하위(부분) 클래스를 나이브 베이즈(naive bayes)라 합니다. 그 이유는 독립성에 대한 가정이 매우 단순하고 나이브하기 때문입니다. 단순한 하위클래스지만 때때로 굉장히 유용하게 사용됩니다. 위의 그림은 나이브 베이즈의 한 예입니다. 나이브 베이즈는 상위 인스턴스와 그에 속한 많은 피쳐노드로 이루어져 있습니다. 여기서는 주어진 인스턴스에 의해 하위노드가 관찰되고 있다고 가정합니다. 즉 인스턴스가 조건으로 설정됩니다. 나이브 베이즈를 사용하여 특정 인스턴스가 속한 제한적인 클래스들 중에서 한 클래스에 대한 추론을 할 수 있습니다. 이 부분은 뒤에서 보다 자세히 설명하겠습니다. 이러한 가정(인스턴스 C에의해 관찰되고 있음, C가 조건으로 설정)을 적용하면, 앞서 다뤘듯이 모든 피쳐($X_1...X_n$)는 조건부 독립입니다. 또한 Class는 피쳐 $X_i$의 상위 노드(부모)이므로 클래스 노드가 관찰되는(Observed)경우 다른 클래스들에 의한 관련성이 차단되고, 다른 클래스와도 독립인 관계가 성립됩니다. 앞서 살펴본 베이지안 네트워크의 체인 룰에 의해 클래스를 포함한 전체 확률 분포는 위의 식과 같이 표현 됩니다.     

![](http://cfile4.uf.tistory.com/image/2413784658951D58063DA4 "Naive Bayes" "width:600px;height:300px;float:center;padding-left:10px;")

이 모델에 대한 이해를 돕기 위해 두 클래스에 대한 비율에 대해 보겠습니다. 앞 슬라이드의 식을 이용한 다음, 분자 분모의 $P(x_1...x_n)$을 소거하면 위 식과 같이 정리 됩니다. 위 식을 보면 두 부분에 의해 나눠 집니다. 앞 부분은 두 클래스에 대한 사전 확률($P(C)$)입니다. 두 번째는 각 클래스에의해 변수가 관찰될 때의 확률 즉 조건부 확률의 곱입니다.  

![](http://cfile9.uf.tistory.com/image/2647C24658951D5908123E "Bernoulli Naive Bayes" "width:600px;height:300px;float:center;padding-left:10px;")

실제 사용되는 텍스트 문서에 대한 예를 보겠습니다. 특정 문서가 있을 때, 이 문서가 애완동물, 금융, 휴가 등 여러 카테고리 중에서 어느 카테고리에 속하는지를 알려고 합니다. 이를 구하기위해 두 나이브 베이스 모델을 사용할 수 있습니다 첫 번째는 베르누이(Bernoulli) 나이브 베이즈입니다. 이 방법은 사전에 있는 모든 단어를 피쳐로 잡은 다음, 문서에 그 피쳐가 나오면 1을 아니면 0인 이진(binary)값을 할당합니다. 따라서 하위 피쳐 노드의 수는 사전에 속한 단어 수(ex. 5,000)와 같습니다.  확률 분포가 0과1 이진 값에 대해 할당되는 베르누이 분포를 띄므로 이 방법을 베르누이 모델이라 부릅니다. 여기서 $P(cat\|Label)$는 특정 Label 문서에서 cat이라는 단어가 나올 확률을 의미합니다. 위의 표와 같이 애완 동물 레이블의 문서의 경우 고양이라는 단어가 나올 확률을 0.3인 반면 금융 레이블의 경우에는 0.001로 매우 낮습니다. buy 나 sell과 같은 금융에 관련된 단어가 등장할 확률은 높을 것입니다. 그리고 $P(word\|Label)$은 앞서 봤듯이 모든 단어에 대해 조건부 독립입니다. 즉 $P(cat\|Label)$과 $P(dog\|Label)$는 서로 완전히 독립적입니다. 이 값은 위 표에 있는 것 처럼 우리가 알 수 있으며 따라서 위 식의 우변에 대입 할 수 있습니다.      

![](http://cfile25.uf.tistory.com/image/2733C54658951D5A37309B "multi-nomial" "width:600px;height:300px;float:center;padding-left:10px;")

두 번째는 다항(multi-nomial) naive base model입니다. 이 모델에서 하위 피쳐 노드는 문서에 있는 단어 입니다. 따라서 피쳐노드의 수는 사전에 있는 단어가 아닌 그 문서에 속한 단어 수입니다. 따라서 문서에 737개 단어가 있다면 피쳐의 수는 737입니다. 그리고 위 그림에 있는 임의 변수($W_i$)에 대한 값은 실제 단어입니다. 사전에 있는 단어 수가 5000개라면 변수($W_i$)에 할당할 수 있는 값 역시 5000개가 될 것입니다. 따라서 변수($W_i$)은 더 이상 이진 값에 대한 확률 분포를 가지지 않습니다. 이는 매우 복잡하게 보일 수 있습니다. 위 표처럼 각 클래스들은 사전에 있는 각 단어에 대한 조건부 확률 분포를 가지고 있어야 합니다. 그리고 당연히 그 값의 합은 1이 됩니다. 그리고 앞에서 살펴 봤듯이 모든 변수에 대한 조건부 확률 분포($P(W_i\|C=c^i)$)는 독립적입니다. 이 값 역시 위 식의 우변에 대입할 수 있습니다. 따라서 특정 문서가 있을 때 그 문서가 속한 카테고리가 어떻게 되는 지를 대략적으로 추측할 수 있습니다.   

![](http://cfile1.uf.tistory.com/image/2113424658951D5B02B1EA "multi-nomial" "width:600px;height:300px;float:center;padding-left:10px;")

나이브 베이즈는 분류 작업에 대해 매우 간단한 접근법을 제공합니다. 이는 효율적이며 구성하기도 쉽습니다. text의 예를 살펴보았지만, 여러 도메인에 대해서도 효과적으로 동작합니다. 하지만 각 피쳐가 독립적이라는 가정을 하고 있으므로, 실제 그들 사이에 관련성이 있는 경우 제대로 동작하기 어려운 면이 있습니다.
