---
title: cs231n 번역 10 Visualizing what ConvNets learn
excerpt: |
 Convolutional Networks를 이해하고 시각화하기위한 여러 가지 접근법 개발되었므여, 이는 신경망에서 학습 된 특성들은 해석 할 수 없다는 일반적인 비판에 대응한다. 이 섹션에서는 이러한 접근법과 관련 작업 중 일부를 간략하게 살펴 본다.   

feature_text: |
  ## Stanford CS class CS231n: 

  Convolutional Neural Networks for Visual Recognition 강의 번역

  ref: [http://cs231n.github.io/](http://cs231n.github.io/ "cs231n")

feature_image: "https://picsum.photos/2560/600/?image=849"
image: "https://picsum.photos/2560/600/?image=849"
comment: true
---


#### Visualizing what ConvNets learn
Convolutional Networks를 이해하고 시각화하기위한 여러 가지 접근법 개발되었므여, 이는 신경망에서 학습 된 특성들은 해석 할 수 없다는 일반적인 비판에 대응한다. 이 섹션에서는 이러한 접근법과 관련 작업 중 일부를 간략하게 살펴 본다. 


##### Visualizing the activations and first-layer weights

**레이어 활성화.** 가장 직관적인 시각화 기술은 순방향 연산 동안 네트워크의 활성화 노드를 보여주는 것이다. ReLU 네트워크의 경우 초반 활성화 노드는 얼룩덜룩한것 처럼 보이며, 상대적으로 작고 밀도가 높지만, 학습이 진행 됨에 따라 점점 밀도가 낮아 지며, 지역적으로 바뀐다. 이 시각화로 쉽게 알아챌 수있는 한 가지 주의 사함은 일부 활성화 맵의 경우 여러 다른 입력에 대해 모두 0 일 수 있으며, 이는 제 기능을 못하는 (dead) 필터를 나타내며, 높은 학습 속도의 징후가 된다.
![](http://cs231n.github.io/assets/cnnvis/act1.jpeg "act1" "width:280px;float:center;padding-left:10px;")![](http://cs231n.github.io/assets/cnnvis/act2.jpeg "act2" "width:280px;float:center;padding-left:10px;") 

고양이 사진 학습에 대한, 첫 번째 CONV 레이어 (왼쪽)와 다섯번째 CONV 레이어의 전형적인 활성화 노드. 모든 박스는 각 필터에 해당하는 활성화 맵이 표시되어 있다. 활성화 맵은 스파스(대부분의 값이  이 시각화에서는 0의 값을 가지는 검은 색으로 표시됨)하며, 대부분 지역적인 특성을 보임에 주목하라. 

**Conv/FC Filters.** 두 번째 일반적인 전략은 웨이트를 시각화하는 것이다. 보통 입력 이미지의 픽셀 데이터와 직접 연결 된 첫 번째 CONV 레이어가 해석에 많은 도움이 되지만, 네트워크 깊은 곳의 필터 웨이트를 볼 수도 있다. 보통 잘 훈련 된 네트워크에서 노이즈 패턴 없이 멋지고 부드러운 필터 웨이트가 나오기 때문에 이를 시각화하는 것은 유용하다. 노이즈 패턴은 충분히 오래 훈련되지 않은 네트워크를 나타낼 수 있으며, 또는 매우 낮은 정규화 강도로 인한 오버피팅의 결과 일 수 있다. 

![](http://cs231n.github.io/assets/cnnvis/filt1.jpeg "filt1" "width:280px;float:center;padding-left:10px;")![](http://cs231n.github.io/assets/cnnvis/filt2.jpeg "filt2" "width:280px;float:center;padding-left:10px;") 

학습 된 AlexNet의 첫 번째 CONV 레이어 (왼쪽) 및 두 번째 CONV 레이어 (오른쪽)의 일반적인 모양의 필터. 첫 번째 레이어 가중치는 매우 멋지고 부드러워서 이 네트워크가 잘 학습 된 것을 알 수 있다. AlexNet에는 2 개로 분리된 프로세싱 스트림이 있으므로, 색상/흑백 특성이 클러스터링된다. 이러한 결과를 통해 한 스트림은 고주파 흑백특성을, 다른 스트림은 저주파 색상 특성을 학습함을 알 수 있다. 두 번째 CONV 레이어 웨이트 맵은 해석하기 어렵지만, 여전히 부드럽고, 잘 형성되어 있으며 노이즈 패턴없는 것을 볼 수 있다..

##### etrieving images that maximally activate a neuron

또 다른 시각화 기법은 큰 이미지 데이터 세트를 가져 와, 네트워크에 넣고 어떤 이미지에서 특정 뉴런이 최대 활성화를 가지는지 보는 것이다. 그런다음 이미지를 시각화하여 뉴런에 대한 수용영역(입력이미지와 연결된 영역)을 볼 수 있다. 이렇나 시각화 중 하나는 Ross Girshick et al.의 [Rich feature hierarchies for accurate object detection and semantic segmentation](http://arxiv.org/abs/1311.2524)에 나와 있다.

![](http://cs231n.github.io/assets/cnnvis/pool5max.jpeg "pool5max" "width:600px;float:center;padding-left:10px;")

AlexNet의 일부 POOL5(5 번째 풀 레이어) 뉴런에 대한 최대 활성화 이미지. 특정 뉴런의 활성화 값과 흰색 테두리의 수용 필드가 표시됨.(특히, POOL5 뉴런은 상대적으로 큰 입력 이미지 영역에 대한 함수임!) 일부 뉴런은 상반신, 텍스트 또는 반사 하이라이트에 반응하는 것을 볼 수 있다.

##### Embedding the codes with t-SNE

ConvNets는 이미지를 선형 분류기로 분리된 카테고리로 점진적 변환하는 것으로 해석 할 수 있다. 이미지를 2 차원으로 임베딩하여 공간의 토폴로지에 대한 대략적인 아이디어를 얻을 수 있다. 각 요소의 거리를 유지하면서, 저 차원 공간에 고차원 벡터를 임베딩하는 많은 방법이 있다. 그 중에서도 t-SNE 는 시각적으로 만족스러운 결과를 꾸준히 주는 가장 잘 알려진 방법 중 하나다.

임베딩을 생성하기 위해 이미지 셋을 가져오고, ConvNet을 사용하여 이를 위한 CNN 코드를 추출 할 수 있다(예 : AlexNet의 경우 분류자 ​​바로 앞의 4096 차원 벡터 및 결정적으로 ReLU 비선형성 포함). 그런 다음 이것을 t-SNE에 연결하고 각 이미지에 대해 2차원 벡터를 얻을 수 있다. 해당 이미지는 격자 형태로 시각화 할 수 있다.

![](http://cs231n.github.io/assets/cnnvis/tsne.jpeg "tsne" "width:600px;float:center;padding-left:10px;")

t-SNE는 CNN 코드를 기반으로 이미지 셋을 임베딩 한다. 서로 가까이에있는 이미지는 CNN 표현 공간에서도 가깝다. 이는 CNN이 매우 유사한 이미지로 간주한다는 의미다. 여기서 유사점은 픽셀 및 색상 보다는 클래스나 의미론적인 부분을 보는 경우가 많다. 이 시각화 방법에 대한 자세한 내용과 관련 코드 및 여러 스케일에 대한 시각화는 [CNN 코드의 t-SNE 시각화](http://cs.stanford.edu/people/karpathy/cnnembed/)를 참조하라. 

##### Occluding parts of the image

ConvNet이 이미지를 개로 분류한다고 가정해보자. 배경이나 기타 물체들과 대조적으로 이미지에서 개를 나타내는 부분을 확인 할 수 있을까? 이미지의 어느 부분이 어떤 클래스로 분류 예측 되는지 조사하는 한 가지 방법은 관심 클래스 (예 : 개 클래스)의 확률을 가려진 영역의 위치 함수로 플로팅하는 것이다. 즉, 이미지의 전 영역을 돌며, 0 값을 가지는 이미지 패치를 설정하고 이때 클래스 확률을 반복적으로 확인하는 것이다. 이제 확률을 2 차원 열지도로 시각화 할 수 있다. 이 접근법은 Matthew Zeiler의 [Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901) 에서 사용되었다.

![](http://cs231n.github.io/assets/cnnvis/occlude.jpeg "occlude" "width:600px;float:center;padding-left:10px;")

개 입력 이미지 (상단). 가려진 영역은 회색으로 표시된다. occluder를 이미지 특정 부분 위에 올렸을 때 올바른 클래스가 표시될 확률을 기록한 다음 히트 맵(각 이미지 아래에 표시됨)으로 시각화. 예를 들어, 가장 왼쪽의 이미지에서 강아지의 얼굴을 덮을 때 Pomeranian의 확률이 낮아져서 강아지의 얼굴은 주로 높은 분류 스코어에 큰 영향을 주는 것을 알 수 있다. 반대로, 이미지의 다른 부분을 영의 값을 가지는 패치로 만드는 것은 상대적으로 무시할만한 영향을 미친다. 

##### Visualizing the data gradient and friends

**Data Gradient.**

[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](http://arxiv.org/abs/1312.6034)

**DeconvNet.**

[Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901)

**Guided Backpropagation.**

[Striving for Simplicity: The All Convolutional Net](http://arxiv.org/abs/1412.6806)

##### Reconstructing original images based on CNN Codes

[Understanding Deep Image Representations by Inverting Them](http://arxiv.org/abs/1412.0035)

##### How much spatial information is preserved?
[Do ConvNets Learn Correspondence? (tldr: yes)](http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf)

##### Plotting performance as a function of image attributes

[ImageNet Large Scale Visual Recognition Challenge](http://arxiv.org/abs/1409.0575)

##### Fooling ConvNets

[Explaining and Harnessing Adversarial Examples](http://arxiv.org/abs/1412.6572)

##### Comparing ConvNets to Human labelers

[What I learned from competing against a ConvNet on ImageNet](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)

