---
title: cs231n 번역 6 Neural Networks Part 2 Setting up the data and the model
excerpt: |
 이전 섹션에서는 뉴런에 대한 비선형성에 따른 내적 계산 모델링과 이 뉴런을 레이어로 배열하는 Neural Networks 모델을 소개했다. 위 모델로 스코어 함수의 형태를 새롭게 정의했으며, 이전 선형 분류 섹션에서 살펴본 간단한 선형 매핑을 확장 시킬 수 있었다.  

feature_text: |
  ## Stanford CS class CS231n: 

  Convolutional Neural Networks for Visual Recognition 강의 번역

  ref: [http://cs231n.github.io/](http://cs231n.github.io/ "cs231n")

feature_image: "https://picsum.photos/2560/600/?image=765"
image: "https://picsum.photos/2560/600/?image=765"
comment: true
---


##### Setting up the data and the model

이전 섹션에서는 뉴런에 대한 비선형성에 따른 내적 계산 모델링과 이 뉴런을 레이어로 배열하는 Neural Networks 모델을 소개했다. 위 모델로 스코어 함수의 형태를 새롭게 정의했으며, 이전 선형 분류 섹션에서 살펴본 간단한 선형 매핑을 확장 시킬 수 있었다. 특히, 신경망은 비선형성을 수반한 연속적인 선형 매핑을 수행한다. 이 섹션에서는 데이터 전처리, 웨이트 초기화 및 로스함수와 관련된 세부적인 모델 디자인 항목에 대해 설명한다.

##### Data Preprocessing
데이터  X를 전처리하는 세 가지 일반적인 방법이 있다. 여기서  X는 [N x D] 여기서 [N: 데이터의 수, D: 차원]의 형태를 가진다고 가정한다.

**Mean subtraction** 은 가장 일반적인 전처리 형식이다. 이 방법은 데이터의 모든 개별 피쳐에 그 평균을 빼는 것이다. 이에 대한 기하학적 해석은 데이터의 모든 차원에 대한 분포의 중심을 원점으로 이동하는 것이다. numpy에서, 이 방법은 다음과 같이 구현된다 X -= np.mean(X, axis = 0). 특히 이미지에서는 편의상 모든 픽셀에서 평균 값을 빼거나(X -= np.mean(X)), 세 색상 채널(RGB)에서 개별적으로 위 작업을 수행한다. 

**Normalization** 는 데이터를 정규화하여 데이터의 모든 차원들에 대한 분포을 엇비슷한 크기로 스케일 하는 것을 의미한다. 이 정규화를 수행하는 두 가지 일반적인 방법이 맀다. 하나는 각 차원을 표준 편차 (zero-centered)로 나누는 것이다 ( X /= np.std(X, axis = 0)). 이 전처리의 또 다른 방법은 각 차원을 정규화하여 차원의 최소 및 최대가 각각 -1과 1이되도록 하는 것이다. 다양한 입력 피쳐가 서로 다른 크기(또는 단위)일 경우 전처리를 적용하는 것이 의미가 있다. 하지만 정규화 된 피쳐들은 학습 알고리즘 내에서 거의 같은 중요성을 가져야한다. 이미지의 경우 픽셀의 상대 크기은 이미 거의 동일하며 범위는 0에서 255 사이이므로 이 추가 전처리 단계를 반드시 수행 할 필요는 없다.


![](http://cs231n.github.io/assets/nn2/prepro1.jpeg "prepro1" "width:600px;float:center;padding-left:10px;")

일반적인 데이터 전처리 파이프 라인. 왼쪽: 기본 2 차원 입력 데이터. 중간: 데이터의 각 차원에서 그 평균을 뺌. 이제 데이터 클라우드는 원점을 중심으로 배치된다. 오른쪽 : 각 차원은 표준 편차에 의해 추가로 크기가 조정된다. 빨간색 선은 데이터의 범위를 나타낸다. 중간에 있는 그림의 경우 x,y의 길이가 같지 않지만 오른쪽 그림의 경우 길이가 같다.

**PCA and Whitening**은 전처리의 또 다른 방법이다. 이 과정에서, 데이터는 먼저 앞에 설명한 것처럼 원점에 배치된다. 그런 다음 데이터의 상관 구조를 보여주는 공분산 행렬을 계산할 수 있다.

```python
# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
```

데이터 공분산 행렬의 (i, j) 요소에는 데이터의 i 번째와 j 번째 차원 간의 공분산 이 저장 된다. 특히 이 행렬의 대각선은 분산을 나타낸다. 또한, 공분산 행렬은 symmetric하며 [positive semi-definite](http://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices) 성질을 가진다. 따라서 데이터 공분산 행렬은 SVD 인수분해를 할 수 있다.

```python
U,S,V = np.linalg.svd(cov)
```

여기서의  U의 각 열은 고유(eigen) 벡터이며, S는 singular value의 1-D 배열이다. 데이터의 상관 관계를 없애기 (decorrelate)위해 eigenbasis를 원점으로 이동 시킨 원래 데이터에 투영(project)한다.

```python
Xrot = np.dot(X, U) # decorrelate the data
```

U의 각 열은 orthonormal(Norm이 1이며 서로 직교) 벡터 집합이므로 basis 벡터로 간주 될 수 있다. 따라서 Projection은 데이터 X의 회전에 해당하며, 고유 벡터가 새로운 축이 된다. 만약  Xrot의 공분산 행렬을 계산한다면, 이제는 대각행렬이 됨을 알 수 있다.  np.linalg.svd의 좋은 특성은 반환 값 U의 열인 eigen 벡터가 eigen value의 크기에 따라 정렬된다는 것이다. 이를 이용하여 상위 소수의  벡터만을 사용하고 데이터에 대한 분산이 없는 차원을 버림으로, 데이터의 차원을 줄일 수 있다. 이것을 [PCA (Principal Component Analysis)](http://en.wikipedia.org/wiki/Principal_component_analysis) 차원 감소 라고도 부른다. 

```python
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
```

이 작업을 수행 한 후에는 크기 [N x D]의 원본 데이터 집합이 크기 [N x 100]로 축소되며, 이 때 데이터에 대한 대부분의 분산을 표현하는 100개 차원에 대한 정보는 지켜진다. PCA로 축소 된 데이터 세트에서 선형 분류기나 신경망을 훈련하면, 저장공간과 시간을 절약하면서 매우 우수한 성능을 얻을 수 있다.

실제로 볼 수있는 마지막 변형은 **화이트닝(whitening)**이다. 화이트닝 연산은 eigenbasis에서 데이터를 취하여, eigen value값으로 모든 차원을 나눔으로써 스케일을 표준화한다. 이 변환의 기하학적 해석은 입력 데이터가 다 변수 가우스 분포인 경우, 제로 평균 및 항등(identity) 공분산 행렬을 갖는 가우시안 분포로 데이터를 화이트닝 하는 것이다. 이 단계는 다음과 같은 방법으로 수행된다. 

```python
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
```

주의: 노이즈 과장. 0으로 나눠지지 않도록 1e-5 (또는 작은 상수)를 추가한다. 이 변환의 한 가지 단점은 입력의 모든 차원(대부분 노이즈인 작은 분산을 가지는 관련이 거의 없는 차원 포함)을 같은 크기로 확장하므로 데이터의 노이즈를 크게 키울 수 있다는 것이다. 실제 사용에서는, 더 강한 평활화(즉, 1e-5를 증가시켜 더 큰 수)를 줘서 이러한 현상을 완화 시킬 수 있다. 

![](http://cs231n.github.io/assets/nn2/prepro2.jpeg "prepro2" "width:600px;float:center;padding-left:10px;")

PCA / 화이트닝. 왼쪽 : 원 2 차원 입력 데이터. 중간: PCA 수행 후. 데이터는 0에 중심이 맞춰지고 데이터 공분산 행렬의 eigenbasis로 회전된다. 이제 각 데이터의 차원들 사이에 상관 관계가 없다.(공분산 행렬은 대각선이 된다.). 오른쪽 : 각 차원은 eigenvalues에 의해 추가로 조정되어 데이터 공분산 행렬을 항등 행렬로 변환한다. 기하학적으로 이것은 등방성 가우시안 블롭으로 데이터를 늘리거나 줄이는 것과 같다.

또한 CIFAR-10 이미지로 이러한 변형을 시각화 할 수 있다. CIFAR-10의 트레이닝 이미지 세트는 각 이미지를 3072 차원의 행 벡터로 확장하여 50,000 x 3072 크기를 가지게 된다. 그런 다음 [3072 x 3072] 공분산 행렬을 계산하고 SVD 분해 (상대적으로 계산량이 많을 수 있음)를 계산할 수 있다. 계산 된 eigen 벡터가 어떻게 보이는가? 이미지가 이해에 도움이 될 수 있다.

![](http://cs231n.github.io/assets/nn2/cifar10pca.jpeg "cifar10pca" "width:600px;float:center;padding-left:10px;")

왼쪽 : 49 개의 이미지 세트 예. 왼쪽에서 두 번째 : 3072 개의 eigen 벡터 중 상위 144개 벡터. 가장 큰 몇몇 eigen 벡터는 데이터의 대부분의 분산을 설명하며 이미지의 낮은 주파수 특성을 나타낸 다는 것을 알 수 있다. 오른쪽에서 두 번째:여기에 표시된 144개의 eigen 벡터를 사용하여 49 개의 이미지를 PCA로 축소함. 즉, 모든 이미지를 3072 차원 벡터로 표현하는 대신, 각 요소가 특정 위치 및 채널에서 특정 픽셀의 밝기를 표현하는 144 차원 벡터로 표현되며, 각 요소와 각 eigen 벡터가 합쳐져서 이미지를 구성한다. 144 개의 숫자에서 어떤 이미지 정보가 유지되었는지 시각화하기 위해 우리는 3072 개의 "픽셀"단위로 다시 회전해야한다. U는 회전이므로 U.transpose () [: 144, :]를 곱한 다음 결과로 얻은 3072 벡터를 이미지로 시각화하면 된다. 이미지가 약간 희미 해지는 것을 볼 수 있습니다. 따라서 이는 상위 고유 벡터가 낮은 주파수를 표현 한다는 사실을 반영한다. 오른쪽 : "화이트닝"의 시각화. 144 차원의 각 분산이 균등한 길이로 축소된다. 여기서, 화이트닝 된 144 개의 숫자에 U.transpose () [: 144 ,:]를 곱하여 이미지 픽셀 단위로 다시 회전한다. 이미지에 대한 대부분의 분산을 설명하는 낮은 주파수(색, 면)은 이제 무시할 수 있는 반면 높은 주파수의 분산이 표현 된다. 

**In practice.**  전처리의 완전성을 위해 PCA / Whitening에 대한 설명을 했지만, Convolutional Networks에는 이러한 변환이 사용되지 않는다. 하지만 데이터를 중앙에 배치하는 것은 매우 중요하며, 일반적으로 모든 픽셀에 대한 정규화를 수행한다. 

**Common pitfall.** 전처리 과정에서 중요한 점은 여기 사용되는 통계(예 : 데이터 평균)를 학습 데이터에서 계산 한 다음 벨리데이션/테스트 데이터에 적용해야 한다는 것이다. 예를 들어 전체 데이터에 대한 평균을 계산하고 전체 데이터 세트의 모든 이미지에서 이 평균은 뺀 다음, 데이터를 train / val / test 스플릿으로 나누는 것은 흔히 저지를 수 있는 실수다. 대신, 평균을 학습 데이터를 통해서만 계산 한 다음, 모든 스플릿 (train / val / test)에서 똑같이 빼야한다.

##### Weight Initialization
뉴럴 네트워크를 구성하는 방법과 데이터를 전처리하는 방법을 보았다. 이제 네트워크를 학습하기 전에 파라미터를 초기화해야한다.

**Pitfall: all zero initialization.** 하지 말아야 할 것을 먼저 알아 보자. 학습 된 네트워크에서 모든 웨이트의 최종 값이 무엇인지 알 수는 없지만, 적절한 데이터 정규화를 기반으로 학습 되었다면, 웨이트의 약 절반은 양수이고 절반은 음수라고 가정하는 것은 타당해 보인다. 이때 합리적으로 생각되는 아이디어는 모든 가중치에 대한 초기 값을 0으로 설정하는 것일 수 있다. 이것은 얼핏 "가장 좋은 추측"처럼 보인다. 하지만 네트워크의 모든 뉴런이 동일한 출력을 계산하게되므로, 역전파 중 동일한 그라디언트가 계산되고, 정확히 동일한 값으로 파라미터 업데이트를 수행하기 때문에 이는 잘못된 결정이다. 즉, 뉴런의 웨이트가 동일하게 초기화되면 네트워크 비대칭성이 사라지게된다. 


**Small random numbers.** 그러므로 여전히 웨이트 값들이 0에 매우 가깝게 설정되길 원하지만, 위에서 논의한 것처럼 동일한 값을 가지지 않아야 한다. 이에대한 해결책으로, 뉴런의 웨이트를 작은 무작위 값으로 초기화하여 대칭성을 깨뜨리는 것이 일반적 입니다. 처음 뉴런 값은 모두 서로다른 무작위값으로 설정 되며 따라서, 서로다른 업데이트 값이 계산 되며, 전체 네트워크의 다양한 부분에 통합 될 것이다. 하나의 웨이트 행렬에 대한 위 방법의 구현은 다음과 같다 W = 0.01* np.random.randn(D,H). 여기서randn은 편균이 0, 표준편차가 1인 가우시안의 무작위 샘플을 의미한다. 공식을 사용하면 모든 뉴런의 웨이트 벡터가 다차원 가우시안 분포에서 샘플링 된 임의의 벡터로 초기화되므로 입력 공간에서 임의의 방향을 나타내게된다. 또한 균일한(uniform) 분포에서 뽑은 무작위 작은 수를 사용할 수도 있지만, 실제로는 최종 성능에 거의 영향을 미치지 않는다.

*경고 : 반드시 작은 숫자로 초기화 하는 것이 더 잘 동작하는 것은 아니다. 예를 들어, 매우 작은 웨이트를 갖는 신경망 레이어는 역전파 중 매우 작은 그라디언트가 계산된다. (이 그라디언트는 웨이트의 값에 비례하므로). 이것은 네트워크를 통해 역방향으로 흐르는 "그래디언트 신호"를 크게 감소시키며, 네트워크가 깊은 경우 문제가 될 수 있다.*

**Calibrating the variances with 1/sqrt(n).** 위의 제안에 대한 한 가지 문제점은 무작위로 초기화 된 뉴런의 출력 분포의 분산이 입력에 따라 증가한다는 것이다. fan-in (즉, 입력의 수, n)의 제곱근으로 웨이트 벡터를 스케일링함으로써 각 뉴런의 출력 분포의 분산을 1로 정규화 할 수 있다. 즉, 각 뉴런의 가중치 벡터를 다음 w = np.random.randn(n) / sqrt(n)과 같이 초기화 하는 것이다. 이는 네트워크의 모든 뉴런을 대략 동일한 출력 분포를 가지게하며, 경험적으로 수렴 속도를 향상시킨다.

다음과 같이 유도 할 수 있다. 여기서 내적 $s = \sum_i^n w_i x_i$는 웨이트 $w$와 입력 $x$의 내적을 의미하며, 비선형성(ex. sigmoid)를 적용전 뉴런의 연산을 의미한다. $s$의 분산은 아래와 같이 구할 수 있다. 

$$\begin{align} \text{Var}(s) &= \text{Var}(\sum_i^n w_ix_i) \\\\ &= \sum_i^n \text{Var}(w_ix_i) \\\\ &= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\\\ &= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \\\\ &= \left( n \text{Var}(w) \right) \text{Var}(x) \end{align}$$

처음 2 스텝은 properties of variance 를 사용했다. 세번째 스텝에서는 입력과 웨이트의 평균이 다음과 같이 0이라고 가정했다. 즉 $E[x_i] = E[w_i] = 0$ 이것은 일반적이 경우는 아니다. 가령 ReLU 유닛의 경우 항상 그 출력에 대한 평균은 0보다 클 것이다. 마지막 스텝에서 웨이트와 입력의 각 원소는 항등 분산을 가진다고 가정하였다. 위에서 $s$와 $x$의 분산이 같도록 하려면, $w$의 분산이 $1/n$이 되어야 한다.  무작위 변수 $X$와 $a$에 대해 $\text{Var}(aX) = a^2\text{Var}(X)$와 같은 식이 성립하므로, 따라서 $X$가 유닛 가우시안 분포를 가질때 $a$는 $a = \sqrt{1/n}$이 되어야 한다. 그러므로 고기화 공식은 다음과 같다.  w = np.random.randn(n) / sqrt(n)

Glorot et al.의 논문 Understanding the difficulty of training deep feedforward neural networks 에서도 이와 유사한 방식을 권하고 있다. 여기서 초기화 형식은 $\text{Var}(w) = 2/(n_{in} + n_{out})$이며, $n_in$, $n_out$은 이전 레이어와 다음 레이어의 입출력 유닛(뉴런)의 수와 같다. 이는 역전파에 대한 분석에 기반하고 있다. 더 이 주제에 대한 더 최근 논문은 He et al.의 Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 이며, ReLU 뉴런에 대한 초기화 식을 유도하여 뉴런의 분산이 $2.0/n$이여야함을 보였다. 

초기화 구현은 다음과 같으며  w = np.random.randn(n) * sqrt(2.0/n), 현재 ReLU 뉴런을 가진 뉴럴 네트워크를 구현에서 초기화 방법으로 권장된다. 

**Sparse initialization.** 보정되지 않은 분산 문제를 해결하는 또 다른 방법은 모든 웨이트 행렬을 0으로 설정(두 뉴런 사이에 연결성이 없음)한 다음, 대칭성을 깨기 위해 뉴런을 그 다음 몇개의 뉴런에 임의로 연결(위에서와 같이 작은 가우스에서 샘플링 된 가중치로)한다. 연결할 뉴런의 일반적인 수는 10개 정도이다. 이렇게 함으로써 웨이트 매트릭스의 대부분 값은 0이며 몇 개만 작은 무작위 값을 가진다. 이로써 대칭성을 제거하며, 분산 또한 크게 늘어 나지 않게 된다.   

**Initializing the biases.** 웨이트 매트릭스가 작은 난수값들에 의해 초기화 되어 비대칭성을 나타내므로, 바이어스는 0으로 초기화 가능하며 일반적으로 그렇게 설정한다. ReLU 비선형성의 경우 모든 바이어스 값에 대해 0.01과 같은 작은 상수 값으로 초기화 하기도 한다. 이는 모든 ReLU 함수가 처음부터 그라디언트를 값을 얻고 이를 전파하도록 보장하기 때문이다. 그러나 이러한 방법이 실제 학습에 효과적인지는 분명하지 않으며(사실 일부 결과는 성능이 나빠지기도 함), 단순히 0으로 바이어스를 초기화하는 것이 더 일반적이다.

***In practice,** 실제 구현에서는 ReLU 유닛을 사용하고 He et al에서 제안한  w = np.random.randn(n) * sqrt(2.0/n) 를 사용하여 초기화 하는 것을 권장한다. 

**Batch Normalization.**  Ioffe와 Szegedy가 최근에 개발 한 기술은  [Batch Normalization](http://arxiv.org/abs/1502.03167)으로 학습 시작시 네트워크에 명시적으로 activations을 가해, 유닛 가우시안 분포를 취하도록 네트워크를 적절히 초기화하여 많은 문제를 해결했다. 핵심은 이러한 정규화 함수는 간단하에 미분할 수 있으므로 이것이 가능하다는 것이다. 구현시 이 방법을 적용하면 일반적으로 완전히 연결 레이어(Fully connected layer) 또는 컨벌루션 레이어와 비선형성이 적용되기 직전에 BatchNorm 레이어를 삽입 할 수 있다. 링크 된 논문에 잘 설명 되었기 때문에 이 기법에 대해 더 깊이 다루지는 않겠지만, 뉴럴 네트워크에서 Batch Normalization을 사용하는 것은 매우 일반적인 방법이 되었다. 실제로 Batch Normalization를 사용하는 네트워크는 잘못된 초기화에 더 원활하게 동작한다. 또한, Batch Normalization은 네트워크의 모든 레이어에서 전처리를 수행하는 것으로 해석 될 수 있지만, 미분 가능하므로 네트워크 자체에 오퍼레이터(레이어)로 직접 통합된다. 

##### Regularization

overfitting을 방지하기 위해 신경망의 capacity를 제어하는 ​​몇 가지 방법이 있다.

**L2 regularization**은 아마도 정규화(Regularization) 의 가장 일반적인 형태 일 것이다. 모든 파라미터에 대한 제곱 값을 목적함수에 직접 페널티로 적용하여 구현할 수 있다. 네트워크의 모든 웨이트 $w$는 $\frac{1}{2} \lambda w^2$과 같은 식으로 목적함수에 더해진다. 여기서 $\lambda$는 정규화 세기를 나타낸다. 일반적으로 식의 앞에 $\frac{1}{2}$가 붙는데, 그 이유는 $w$에 대한 미분 값이 $2 \lambda w$가 아닌 간단히 $\lambda w$로 표시되길 원하기 때문이다. L2 정규화는 직관적으로 피크값을 가지는 벡터에 심한 페널티를 가하며, 웨이트 벡터가 확산되는 것을 선호한다. 따라서 Linear Classification 섹션에서 논의했듯이, 네트워크에서 웨이트와 입력 간의 곱셈이 이루어 지므로, 위 정규화는 입력 중 몇몇만 반영하기 보다는 모든 입력을 사용하도록 하는 매력적인 특성이 있다. 
마지막으로 L2 정규화에 대한 그라디언트 디센트 파라미터 업데이트는 결국 모든 웨이트를 선형적으로 감소시킨다.  W += -lambda * W  

**L1 regularization**은 정규화의 또 다른 일반적인 형태이다. 여기서는 파라미터 $w$에 대한 $\lambda  \mid w \mid$를 목적함수에 더해준다. 다음과 같이 L1과 L2 정규화를 함께 사용 할 수도 있다. $\lambda_1 \mid w \mid + \lambda_2 w^2$ 이를 Elastic net regularization이라 부른다. L1 정규화에는 최적화 과정에서 웨이트 벡터를 sparse하도록 유도하는 흥미로운 속성이 있다 (즉 0에 가깝게 만든다). 즉, L1 정규화를 갖는 뉴런은 가장 중요한 입력에 대한 몇몇 부분만 사용하여 결국 "노이즈"입력을 거의 반영하지 않게 된다. 이에 반해, L2 정규화의 최종 웨이트 벡터는 보통 확산되고 작은 값을 가진다. 실제로, 명시적인 함수 선택을 고려하지 않는다면, 일반적으로 L2 정규화는 L1보다 우수한 성능을 제공 할 것으로 기대된다. 

**Max norm constraints.** 정규화의 또 다른 형태는 모든 뉴런에 대한 웨이트 벡터의 크기에 절대적인 상한을 적용하고 이러한 제약을 적용하기 위해 그래디언트 디센트를 사용하는 것이다. 실제로 이는 정상적으로 파라미터 업데이트를 수행한 다음 모든 뉴런에 대해 $\Vert \vec{w} \Vert_2 < c$가 만족되도록 $w$를 clamping 하는 것과 같다. $c$는 일반적으로 3이나 4를 사용한다. 몇몇 경우 이러한 형태의 정규화를 사용함으로써 이득은 얻는다. 특히 그 매력적인 속성 중 하나는 학습 속도가 너무 높게 설정되어 있어도, 네트워크에는 상한선이 있으므로 발산하지 않는다는 점이다. 

**Dropout**은 비교적 최근에 Srivastava(Dropout: A Simple Way to Prevent Neural Networks from Overfitting) 제안으로 도입 되었으며, 매우 효과적이고 간단한 정규화 기술로 다른 방법들(L1, L2, maxnorm)을 보완한다. 학습 과정 동안, 드롭아웃은 특정 확률 $p$(hyperparameter)로 뉴런을 활성화 상태로 유지하거나, 그렇지 않으면 0으로 설정한다. 

![](http://cs231n.github.io/assets/nn2/dropout.jpeg "dropout" "width:600px;float:center;padding-left:10px;")

알고리즘을 설명하는 드롭 아웃 Paper의 그림. 학습 과정 동한 드롭아웃은 전체 신경망 내에서 특정 튿정 뉴런을 포함하는 작은 네트워크로 샘플링하고, 입력 된 데이터를 기반으로 표본 네트워크의 매개변수만 업데이트하는 것으로 해석 할 수 있다. (그러나 샘플링 된 다양한 네트워크들은 파라미터를 공유하기 때문에 독립적이지는 않다.) 테스트 중에는 드롭아웃을 가하지 않으며, 모든 서브 네트워크의 앙상블을 통한 평균적 예상값을 사용한다. 

3-레이어 신경망에서 기본 드롭 아웃은 다음과 같이 구현된다.

```python
""" Vanilla Dropout: Not recommended implementation (see notes below) """

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  """ X contains the data """
  
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # first dropout mask
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # second dropout mask
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3
```

위의 코드에서 train_step함수 내부에서 첫 번째 히든 레이어와 두 번째 히든 레이어에 대한 두 번의 드롭 아웃을 수행했다.  입력 레이어  X에서 바로 드롭 아웃을 수행 할 수도 있다. 이 경우 입력에 대한 바이너리 마스크도 생성한다 . 역방향 패스는 바뀌지 않고 유지되지만, 물론 생성 된 마스크 U1,U2를 고려해야한다.



결정적으로, 예측 함수에서는 더 이상 dropping을 수행하지는 않지만 히든 레이어 출력을 p로 스케일링한다. 이는 테스트 시에 모든 뉴런이 모든 입력을 받기 때문데 중요하다. 테스트 시에 뉴런의 출력이 학습 시 예상 출력과 같아지기를 원하기 때문이다. 예를 들어, $p = 0.5$ 인 경우, 뉴런은 테스트 시에 출력이 절반으로 줄고 이때, 학습 (기대치) 동안한 것과 결과를 가져야한다. 이해를 위해 드롭아웃 전의 출력 x를 생각해보자. 확률 $1-p$에서 0이 되어야 하므로 드롭아웃의 식은 다음과 같다.($px + (1-p)0$). 테스트 시에 모든 뉴런은 항상 활성화 상태로 유지되며, 이때 기존과 동일한 예상 출력을 유지하려면 $x → px$로 조정해야한다. 

위에서 제시 한 스키마의 바람직하지 않은 특성은 테스트 시에 $p$에 따라 스케일링을 수행한다는 것이다. 테스트 시간 성능은 매우 중요하기 때문에, 학습 시에 스케일링을 수행하는 **역 드롭 아웃 (inverted dropout)** 을 사용하는 것이 바람직 하며 테스트 시 순방향 패스는 그대로 둔다. 또한 드롭 아웃을 적용한 위치를 조정할 때 또는 예상치 못한 부분에서도 예측 코드를 그대로 유지할 수있는 매력적인 속성이 있다. 역 드롭 아웃은 다음과 같다.

```python
"" 
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
"""

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

처음 드롭아웃이 소개 된 이후 실제 사용에서 성능의 근원에 대한 이해 및 다른 정유화 와의 관계에 대한 연구가 많이 있었다. 관심있는 독자를위한 추가 읽을 거리 :

- [Dropout paper by Srivastava et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
- [Dropout Training as Adaptive Regularization](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)

**Theme of noise in forward pass.** 네트워크의 순방향 패스에서 stochastic 동작을 도입하는 보다 일반적인 방법으로 분류된다. 테스트 동안 노이즈는 분석적($p$를 곱하는 경우) 또는 수치적으로 (예 : 표본 조사를 통해, 다른 무작위 결정으로 여러 가지 순방향 통과를 수행 한 다음 평균을 통해)처리 된다. 이러한 방향에서 다른 연구의 예로는 DropConnect가 있다. 여기서 DropConnect는 앞으로 전달되는 동안 임의의 웨이트 세트가 0으로 설정된다


**Bias regularization.** 선형 분류 (Linear Classification) 섹션에서 이미 언급했듯이, 바이어스 파라미터는 데이터와 상호 작용하지 않으므로 바이어스 파라미터에 대한 정규화를 진행하지 않아도 된다. 그러나 실제 응용 프로그램 (및 적절한 데이터 사전 처리)에서 바이어스를 정규화 하면 드물게 성능이 현저하게 저하되는 경우도 있다. 

**Per-layer regularization.** 서로 다른 레이어를 서로 다른 값으로 정규화하는 것은 일반적이지 않다 (출력 레이어 제외). 이 아이디어와 관련한 비교적 나은 결과가 paper에 발표되다.

**In practice:** Cross validation 된 하나의 글로벌 L2 정규화를 사용하는 것이 가장 일반적이다. 이것을 모든 레이어 뒤에 적용된 드롭 아웃과 결합하는 것도 일반적이다. 보통 정규화 세기 $p=0.5$가 합리적인 기본값이며, Cross validation을 통해 조정할 수 있다.


##### Loss functions
모델의 복잡성에 대한 불이익(오버피팅)을 줄이는 것 목적함수의 정규화 로스 부분에 대해 논의했다. 로스함수를 구성하는 다른 한 함수는  데이터 로스 로서, 감독 학습(supervised learning)문제에서 예측 (예 : 분류의 등급 점수)과 참값 간의 관련성을 측정한다. 데이터 로스는 모든 개별 샘플의 로스에 대한 평균과 같다. 즉 $L = \frac{1}{N} \sum_i L_i$이며, 여기서 $N$은 학습 데이터 수다. $f = f(x_i; W)$를 뉴럴 네트워크 출력 레이어의 활성화 함수라고 할때, 실제 구현에서 문제에 따라 몇가지 타입이 있다. 

**Classification** 은 지금까지 많이 다뤘었다. 여기서는 각 샘플에 대해 샘플의 데이터 세트와 올바른 레이블이 있다고 가정한다. 여기서 가장 일반적으로 사용되는 비용(cost) 함수 중 하나는 SVM이다 (예 : Weston Watkins 공식).
$$L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)$$
앞에서 언급했듯이, 몇볓 경우에는 제곱 힌지 로스($\max(0, f_j - f_{y_i} + 1)^2$)가 더 나은 성능을 보일 때도 있다. 일반적으로 사용되는 다른 선택으로는 크로스 엔트로피 손실을 사용하는 Softmax 분류기가 있다. 
$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$$

**Problem: Large number of classes.** 레이블 집합이 매우 큰 경우 (예 : 영문 사전의 단어 또는 22,000개의 레이블이 있는 ImageNet) Hierarchical Softmax 를 사용하면 도움이 될 수 있다 ( 여기 pdf 참조). Hierarchical  softmax는 레이블을 트리 구조로 분해한다. 그런 다음 각 레이블은 트리를 따라 경로로 표시되며 Softmax 분류기는 트리의 모든 노드에서 트레이닝되어 왼쪽 분기와 오른쪽 분기를 명확하게 한다. 트리의 구조는 성능에 크게 영향을 미치며 일반적으로 문제에 따라 다르다.


**Attribute classification.** 위의 두 문제에서는 하나의 정답 $y_i$가 있다고 가정한다. 하지만 여기서 $y_i$가 특정 속성의 유무를 표현하는 이진 벡터이며 베타적인 성격을 가진다면 어떻게 될까? 예를 들어, Instagram 이미지에 포함된 해시태그들은 모든 해시 태그를 표현하는 큰 집합에서 해시 태그의 특정 하위 집합으로 분류 된 것으로 생각할 수 있다. 이 경우 합리적인 방법은 모든 단일 속성(해시태그)에 대해 독립적으로 이진 분류자를 작성하는 것이다. 예를 들어, 각 카테고리의 바이너리 분류기는 독립적으로 다음과 같은 형식을 가진다.

$$L_i = \sum_j \max(0, 1 - y_{ij} f_j)$$

여기서 모든 속성 카테고리 $j$의 합이 계산되며, $y_{ij}$는 i번째 샘플의 j번째 속성을 나타나요 +1또는 -1로 레이블링된다. 스코어 함수 $f_j$는 속성이 있을 경우 양수값, 아닐 경우 음수 값을 가진다. 따라서 레이블 된 값과 스코어 함수의 속성이 다를 경우 로스는 축적되며, 부호가 같더라고 그 크기가 1보다 작은 경우 역시 축적된다.  

이 로스 함수의 대안으로 모든 속성에 대한 로지스틱 회귀 분류기를 독립적으로 학습하는 방법이 있다. 이진 로지스틱 회귀분류기는 두 개의 클래스 (0,1) 만 가지며 클래스 1에 대한 확률을 다음과 같이 계산한다.

$$P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)$$

1에 대한 확률과 0에 대한 확률의 합은 1이므로, 0에 대한 확률은 다음과 같다. $P(y = 0 \mid x; w, b) = 1 - P(y = 1 \mid x; w,b)$. 로스 함수는 이 확률의 로그 likelihood를 최대화한다. 따라서 위 두가지 경우를 합쳐 다음과 같이 단순화 할 수 있다. 

$$L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))$$

여기서 라벨 $y_{ij}$는 1또는 0의 값을 가진다. 그리고  $\sigma(\cdot)$는 시그모이드 함수를 나타낸다. 위의 표현은 복잡한 것처럼 보일 수 있지만 $f$의 그라디언트는 실제로 다음과 같이 매우 간단하고 직관적이다.

$\partial{L_i} / \partial{f_j} = y_{ij} - \sigma(f_j)$ , 직접 유도해보자. 

**Regression** 은 주택 가격이나 이미지에 있는 무언가의 크기와 같은 실제 값을 예측하는 작업이다. 이 작업에서는 예측 된 값과 실제 값 사이의 로스를 계산 한 다음 L2 제곱 놈(norm) 또는 차이의 L1 놈을 측정하는 것이 일반적이다. L2 norm 제곱의 경우 단일 예제에 대한 로스는 다음과 같다. 

$$L_i = \Vert f - y_i \Vert_2^2$$

L2 norm이 목적함수에서 제곱 된 이유는 제곱은 단조로운 연산이기 때문에 최적 파라미터를 변경하지 않고, 그라디언트 역시 훨씬 간단해지기 때문이다. L1 놈은 각 차원에 따라 절대 값을 합산한다. 

$$L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid$$

여기서 $\sum_j$는 예측하는 레이블이 둘 이상일 경우 모든 차원에 대한 합을 나타낸다. i 번째 샘플의 j 번째 레이블의 참값과 예측 값의 차이를 $\delta_{ij}$로 나타낼때, 여기에 대한 그라디언트 $\partial{L_i} / \partial{f_j}$는 L2 norm이나 $sign(\delta_{ij})$ 경우 쉽게 유도 된다. 즉, 스코어 함수의 그라디언트는 차에 직접 비례하거나, 부호만 다른 고정된 값만 상속받는다.



주의 사항: L2 로스는 Softmax와 같은 안정적인 로스함수 보다 최적화하기가 훨씬 더 어렵다는 점에 유의해야한다. 직관적으로, 각 입력에 대해 정확히 하나의 올바른 값을 출력해야하는 네트워크의 매우 취약한 특성이 있다. 반면 Softmax에서는 각 스코어의 정확한 값이 덜 중요하다는 사실을 알 수 있다. 스코어가 적절한 크기만 가지면 된다. 또한, 예외값(outlier)가 큰 그라디언트를 유발 할 수 있기 때문에 L2 손실은 덜 범용적이다. 회귀(Regression) 문제에 직면했을 때, 먼저 출력을 양자화(bin)하는 것이 부적절한지 고려하라. 예를 들어, 제품의 별 등급을 예측하는 경우 회귀 로스 대신 1-5 개의 별 등급에 대해 5 개의 독립 분류기를 사용하는 것이 훨씬 효과적 일 수 있다. 분류기는 단일 출력뿐만 아니라 회귀 출력에 대한 분배를 포현하는 추가적인 이점이 있다. 분류기를 사용하는것이 적절하지 않다는 것이 확실하다면, L2를 사용해야 하지만 주의해야합니다. 예를 들어, L2는 앞서 언급한 취약함이 있으며, 네트워크에 드롭 아웃을 적용하는 것은 그리 좋은 생각이 아니다. 


 *회귀를 이용하여 문제를 다룰때, 이 것이 절대적으로 필요한지 먼저 고려하자. 가능한 출력을 이산화하고 분류를 수행하는 것이 좋다.*



**Structured prediction.** 구조적 로스는 레이블이 그래프, 트리 또는 기타 복잡한 임의의 구조가 될 수 있는 경우를 나타낸다. 보통 그래프와 트리 구조는 매우 커서 쉽게 열거 할 수 없다고 가정한다. 구조화 된 SVM 로스의 기본 아이디어는 참 구조($y_i$)와 틀리지만 가장 큰 스코어를 가지는 구조 사이의 마진을 구하는 것이다. 일반적인 그라디언트 디센트를 사용한 최적화만으로 이 문제를 푸는 것은 쉽지 않다. 대신 구조의 가정을 통해 단순화를 도입한 특정 방법을 사용한다. 구체적인 내용은 수업의 범위를 벗어나므로 간략하게만 언급한다. 

##### Summary
요약하자면

- 전처리시, 데이터의 중심값을 0으로 만들고, 각 피쳐를 따라 눈금을 [-1, 1]로 정규화하는 것을 권장한다. 
- 웨이트는 가우시안 분포를 사용하여 초기화 하며, 이때 $\sqrt{2/n}$으로 나눈다. 여기서 n은 입력 뉴런의 수이다. numpy예 : w = np.random.randn(n) * sqrt(2.0/n).
- L2 정규화 및 드롭 아웃 사용
- 배치 정규화(batch normalization) 사용
- 실제로 사용되는 문제에 대한 다양한 경우과 각 경우에서 가장 일반적인 로스함수에 대해 설명했다.

이제 데이터를 전처리하고 모델을 설정 및 초기화했다. 다음 섹션에서는 학습 과정과 그 역학관계을 살펴 볼 것이다.

