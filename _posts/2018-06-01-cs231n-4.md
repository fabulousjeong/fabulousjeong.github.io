---
title: cs231n 번역 4 Backpropagation, Intuitions
category: cs231n
excerpt: |
 Motivation. 이 섹션에서는 역전파(back propagation)의 직관적인 이해를 다룬다. 역전파는 체인 룰(Chain Rule)을 재귀적으로 적용하여 그라디언트를 계산하는 방법이다. 신경망을 이해하고 효과적으로 개발, 디자인 및 디버깅하려면 이 과정과 그 미묘함을 이해하는 것이 중요하다.


feature_text: |
  ## Stanford CS class CS231n: 

  Convolutional Neural Networks for Visual Recognition 강의 번역

  ref: [http://cs231n.github.io/](http://cs231n.github.io/ "cs231n")

feature_image: "https://picsum.photos/2560/600/?image=849"
image: "https://picsum.photos/2560/600/?image=849"
comment: true
---


##### Introduction

**Motivation.** 이 섹션에서는 역전파(back propagation)의 직관적인 이해를 다룬다. 역전파는 체인 룰(Chain Rule)을 재귀적으로 적용하여 그라디언트를 계산하는 방법이다. 신경망을 이해하고 효과적으로 개발, 디자인 및 디버깅하려면 이 과정과 그 미묘함을 이해하는 것이 중요하다.

**Problem statement.** 이 절에서 다룰 핵심 문제는 다음과 같다. 함수 $f(x)$와 입력 벡터 $x$가 있을 때, $x$에 대한 $f(x)$의 미분값(gradient: $\nabla f(x)$)을 계산하는 것이다. 

**Motivation** 위 문제에 관심을 가지는 이유는 특정 신경망에서, $f$는 로스함수에 해당되며 입력 $x$는 트레이닝 데이터와 신경망의 웨이트로 수어되어있기 때문이다. 예를들어 앞에서 로스 값은 SVM 로스 함수와 입력(학습데이터: $(x_i,y_i), i=1 \ldots N$)와 웨이트, 바이어스($W, b$)를 통해 구해진다. 일반적인 Machine Learning과 같이 여기서도 트레이닝 데이터는 고정 된 것으로 생각하고, 웨이트는 조정 할 수 있는 변수로 생각한다. 따라서 역전파를 사용하여 사용하여 입력 샘플($x$)에 대한 그라디언트를 쉽게 계산할 수 있지만, 실제로는 파라미터 업데이트를 수행하기 위해서 파라미터($W, b$)에 대한 역전파만 계산한다.  하지만 클래스 후반에서 볼 수 있듯이 $x_i$에 대한 그라디언트는 신경망 동작에 대한 시각화 및 해석에 유용하게 사용 할 수 있다.


##### Simple expressions and interpretation of the gradient

보다 복잡한 표현을 다루기전에 표기법 및 규칙을 이해하기 위한 간단한 예를 보자. 여기 두 수의 곱으로 표현된 간단한 함수 $f(x,y) = x y$가 있다. 둘 중 어느 쪽의 입력에 대해서도 편미분을 간단하게 유도 할 수 있다. 

$$f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x$$

**Interpretation.** 편미분이 무엇을 의미하는지 유의하라. 특정 지점 근처를 둘러싼 극히 작은 영역의 변수에 대한 함수의 변화율을 나타낸다.

$$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$

위 식에서 왼쪽에 있는 분수 기호(ㅡ)는 오른쪽에 있는 분수 기호와 달리 나눗셉을 나태내지 않는다. 대신 연산자($\frac{d}{dx}$)를 함수 $f$에 적용하여, 함수에 대한 미분을 나타낸다. 위 식에서 $h$가 매우 작을 경우(확대해서 보면) 함수 $f$는 마치 직선처럼 보이며, 미분값은 그 기울기와 같을 것이다. 즉, 각 변수에 대한 미분 값은 전체 표현식이 그 값에 대해 얼마나 민감하게 변하는지를 말해준다. 예를 들어, $x = 4, y = -3$, $f(x,y) = -12$  일 때, $x$에 대한 미분 값은 $\frac{\partial f}{\partial x} = -3$이다. 이것은 $x$를 조금 증가시키면 전체 함수 값은 미분 값이 음수이므로 감소하며, 그 양은 증가시킨 값의 3배와 같음을 의미한다. 위의 방정식을 다음과 같이 정리할 수 있다. 

$$f(x + h) = f(x) + h \frac{df(x)}{dx}$$.

비슷한 방식으로, $\frac{\partial f}{\partial y} = 4$이므로 $y$를 $h$만큼 매우 조금 증가시키면, 함수의 전체 값은 $4h$만큼 증가하게 될 것이다.  

각 변수의 미분은 그 값에 대한 전체 식의 민감도를 나타낸다.

언급 한 바와 같이, 그라디언트 $\nabla f$는 편미분으로 구성된 벡터이므로, $\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]$로 나타낼 수 있다. 그라디언트는 엄밀히 벡터지만, 학술적으로 올바른 표현인 "x의 편미분" 대신 "x의 그라디언트"와 같은 표현을 사용하기도 한다. 또한 덧셈 연산을 위한 편미분을 유도 할 수 있다.

$$f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1$$

즉, x, y에 대한 미분은 x, y의 값이 무엇인지에 관계없이 1이다. 당연하게도, 이것은 x나 y중 하나를 증가시키면 전체 함수값 역시 증가함하며 그 비율은 x나 y의 값과 상관 없음을 말한다. 신경망에서 꽤 많이 다룰 마지막 함수는 max 연산이다. 
$$f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)$$

즉, 여기서 서브그래디언트(subgradient)는 입력 다른 입력 보다 큰 경우 1이며 그렇지 않은 경우 0이다. 즉 입력이 $x = 4,y = 2$인 경우, max 함수 값은 4이며 y 값의 변화에 대한 민감도는 없다(0). 즉 y에 대한 그라디언트가 0이므로 y를 h만큼 증가시켜도 함수 결과는 4를 유지한다. 물론 y를 매우 크게 변화 시키면(2이상 증가)시키면 $f$ 값은 바뀌게 된다. 하지만 편미분 값은 함수의 입력에 큰 변화의 영향에 대해 다루지 않는다. 식의 정의에 $\lim_{h \rightarrow 0}$과 같이, 편미분은 입력에 대한 극소 변화에 대해서만 다룬다. 

##### Compound expressions with chain rule

지금 부터는 다음과 같이 $f(x,y,z) = (x + y) z$ 여러 함수로 구성된 보다 복잡한 표현식을 살펴보겠다. 이 표현식은 여전히 ​​직접 미분해도 될 정도로 간단하지만, 역전파를 직관적으로 이해하는 데 도움이되는 접근 방식을 통해 미분 할 것이다. 특히,이 표현식은 두 개의 표현식($q = x + y$, $f = q z$)으로 표현 할 수 있다. 게다가 이전 절에서 두 표현식의 편미분를 계산하는 방법을 이미 보았다. $f$는 $q, z$의 곱이므로 $\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q$이며, $q$는 $x, y$의 합이므로 $\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1$이다. $\frac{\partial f}{\partial q}$는 사용하지 않으므로, $q$에 대한 편미분은 구할 필요가 없다. 오직 $x, y, z$에 대한 $f$의 그라디언트만 관심있을 뿐이다. 체인 룰은 다음과 같이 그라디언트를 연결(chain) 된 곱셈으로 표현 한는 좋은 방법을 제시해 준다.  $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}$. 실제 이 계산은 그라디언트를 나타내는 두 수의 간단한 곱으로 이뤄진다. 다음 예를 살펴보자.  

```python
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q * z # f becomes -12

# perform the backward pass (backpropagation) in reverse order:
# first backprop through f = q * z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
# now backprop through q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!
dfdy = 1.0 * dfdq # dq/dy = 1
```

마지막 [dfdx, dfdy, dfdz] 변수에 그라디언트를 담아, 변수 x, y, z에 대한 f의 민감도를 구한다. 이것은 역전파의 가장 간단한 예시다. 앞으로는 보다 df 없애 보다 간결하게 표기하고자 한다. 즉, dfdq 대신 dq로 표현하며, 항상 그래디언트는 최종출력을 기준으로 계산 된다고 가정한다.
이 계산은 회로도로 멋지게 시각화 할 수 있다.

![](http://cfile2.uf.tistory.com/image/99EDA83F5A65E5461F7A3A "loss" "width:600px;float:center;padding-left:10px;")

왼쪽의  "회로" 는 계산과정의 시각적으로 표현한다. **포워드 패스**(녹색으로 표시)는 입력에 대한 출력 값을 계산한다. **백워드 패스**는 끝단에서 시작하여  재귀적으로 체인룰을 사용해 편미분 값을 계산하는 역전파를 수행한다. 그라디언트는 회로 따라 거꾸로 흐른다고 생각할 수 있다.

##### Intuitive understanding of backpropagation

역전파는 아름다운 로컬 프로세스임을 명심하자. 회로 다이어그램의 모든 게이트는 몇 가지 입력을 가지며 즉시 두 가지를 계산할 수 있다. 1. 출력 값 2. 입력에 대한 출력값의 로컬(Local) 그래디언트. 게이트는 전체 회로의 세부 사항과 상관없이 위 계산을 완전히 독립적으로 수행 할 수 있다. 순방향 패스가 끝나면, 전체 회로의 최종 출력으로 부터 시작된 역전파 과정을 통해 게이트는 출력 값의 그래디언트를 계산한다. 체인 룰을 통해 입력에 대한 그래디언트는 그 입력 값으로 부터 파행된 모든 모든 게이트의 그래디언트를 곱해서 구할 수 있다.  

*체인 룰을 통한 추가 곱셈 (각 입력에 대한)은 신경망과 같은 복잡한 회로에서 상대적으로 쓸모없는 게이트를 톱니바퀴의 톱니 처럼 여길수 있다.* 

위 예제를 다시보며 작동하는 방식에 대한 직관을 얻어 보도록 하자. 합연산 게이트는 입력 [-2, 5]로 부터 출력 3을 계산했다. 게이트가 합 연산을 수행하기 때문에 두 입력에 대한 그 로컬 그래디언트는 +1 이다. 나머지 곱연산 게이트는 최종 값인 -12를 계산했다. 위에서 합연산 게이트의(곱연산 게이트에 대한 입력)는 출력에 대한 그래디언트는 -4 다. 회로를 의인화해보면, 회로에서 합연산 게이트의 출력이 낮아지기(음의 부호로 인해, 4배 만큼)를 바라는 것으로 생각할 수 있다. 체인 룰을 통해 그래디언트를 연결하여(입력 게이트의 로컬 그라디언트와 합연산 게이트의 그라디언트를 곱함) 입력에 대한 전체 출력의 그라디언트를 구할 수 있다.( x 및 y 둘 모두 그라디언트는 1 * -4 = -4). 이것은 다음과 같은 효과가 있다. x, y가 감소하면(음수 그래디언트에 응답) 합연산 게이트의 출력이 감소하여 결국 곱셈 게이트의 출력이 증가한다.

따라서 위 예에서 역전파는 게이트 간의 통신을 통해, 개별 게이트에 대한 로컬 출력의 증가/감소와 여부와 상관없이 최종 출력 값을 높이도록 동작한다. 

##### Modularity: Sigmoid example

위에서 소개 한 게이트들은 상대적으로 단독적이다. 미분가능한 어떤 함수라도 게이트 역할을 할 수 있으며, 여러 게이트를 단일 게이트로 그룹화하거나, 편의에 따라 함수를 여러 게이트로 분해 할 수 있다. 이를 보여주는 또 다른 표현식을 살펴보겠다.

$$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$

수업 후반부에 다루겠지만, 위 식은 시그모이드 활성화 함수를 사용하는 2 차원 뉴런(입력 x 및 가중치 w)을 표현한다. 지금은 이것을 매우 간단한 입력 w, x 에서 하나의 숫자를 출력하는 함수로 생각하자. 이 함수는 여러 개의 단독 게이트로 구성될 수 있다. 위에서 설명한 것들 (add, mul, max) 외에도 네 가지가 더 있다.
$$f(x) = \frac{1}{x} \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = -1/x^2 \\\\ f_c(x) = c + x \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = 1 \\\\ f(x) = e^x \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = e^x \\\\ f_a(x) = ax \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = a$$
여기서 $f_c$는 상수 c만큼 입력은 이동시키며, $f_a$는 상수 a만큼 입력을 확대/축소한다. 두 함수는 각각 합연산과 곱연산의 특별한 경우라고 볼 수 있다. 하지만 상수 c, a에 대한 그라디언트가 필요하기 때문에 소개했다. 위 시그모이드 함수에 대한 전체 회로도는 아래와 같다. 

![](http://cfile7.uf.tistory.com/image/99C4673F5A6734180895BC "sigmoid" "width:600px;float:center;padding-left:10px;")

시그모이드 활성화 함수를 사용하는 2D 뉴런에 대한 회로도. 입력은 [x0, x1]이고 뉴런의(학습 가능한) 웨이트는 [w0, w1, w2]다. 나중에 보게 되겠지만, 뉴런은 입력에 대한 내적을 계산 한 다음 시그모이드 함수에 의해 0에서 1사이의 범위 내에서 부드럽게 압축된다.

위 예제에서 w, x의 내적에 대한 시그모이드 함수($\sigma(x)$)의 긴 체인을 볼 수 있다. 시그모이드 함수의 편미분이 매우 간단하다는 사실을 쉽게 유도할 수 있다. (분자에 1을 더하고 뺌).
$$\sigma(x) = \frac{1}{1+e^{-x}} \\\\ \rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)$$
위에서 보았듯이 그라디언트는 놀랍도록 간단해진다. 예를 들어, 시그모이드식은 입력으로 1.0(x와 w의 내적 결과)을 받고 순방향 전달을 통해 최종 출력 0.73을 계산한다. 위의 회로도에서 로컬 그래디언트는 계산 된 편미분 값(1 - 0.73) * 0.73 - = 0.2과 같음을 보여준다. 따라서 실제 구현에서 이러한 그라디언트 계산 작업은 단일 게이트로 그룹화하는 것이 체인룰을 이용하는 것보다 훨씬 유용하다. 코드를 통해 살펴보자.

```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```

**Implementation protip: staged backpropagation.** 위의 코드에서 볼 수 있듯이 실제 구현에서는 순방향 패스의에 중간 단계을 둬, 역전파를 계산을 쉽게 하도록 만드는 것이 좋다. 예를 들어 여기서는 두 입력에 대한 내적의 출력값을 유지하는 중간 변수( dot) 를 만들었다. 이를 사용하여 역전파 동안 여기에 대응 되는 변수( ddot) 및 최종적으로는 그라디언트(dw, dx)를 계산한다. 

이 섹션의 요점은 역전파가 수행되는 세부적인 방식이며 전달 함수의 어떤 부분이 게이트로 구성되었는지는 편의상 문제라는 것이다. 전체 회로를 단독게이트 들로 구성할 필요는 없다. 코드와 계산은 간결게 만들기 위해, 쉬운 로컬 그라디언트를 가지도록 연산식을 구성하고 서로 연결한다.  

##### Backprop in practice: Staged computation
다른 예로 다음과 같은 함수가 있다고 가정 해보자.

$$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$

사실 머신러닝에서 위 함수는 전적으로 쓸모 없으며, 위 함수의 그라디언트를 구하는 일은 없을 것이다. 하지만 위 함수는 역전파를 이해하는데 좋은 예는 될 수 있다. 위와 같이 매우 복잡한 함수의 x 나 y의 편미분을 구하는 일은 매우 어려운 작업일 것이다. 하지만 앞에서도 보았듯이 정확한 그라디언트 함수가 반드시 필요하지는 않으므로, 이러한 작업을 수행하지 않아도 된다. 그라디언트가 어떻게 계산되는지만 알면 된다. 아래 코드는 위 식이 어떻게 정방향 연산(forward pass)을 하는지 나타낸다.      

```python
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)
```

마지막 줄에서 드디어 정방향 연산이 끝났다. 위에서 이미 다룬 간단한 연산자와 그것으로 부터 계산된 변수를 저장하게끔 코드를 구조화 했다. 따라서 역전파를 계산하는 것은 쉽다. 역방향으로 진행하며 정방향 연산( sigy, num, sigx, xpy, xpysqr, den, invden)에서 계산된 모든 변수에 대해, 최종 출력에 대한 기울기를 저장(이때 대응 되는 변수는 동일한 이름을 가지지만 앞에 d가 붙음)한다. 또한, 모든 단독 연산자에 대해 로컬 그래디언트를 계산하고 체인룰을 통해 앞에서 계산된 그라디언트와 곱해 최종 출력에 대한 그라디언트를 구한다. 아래 코드의 각 행마다 대응되는 정방향 연산을 표시했다.

```python
# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den 
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
# done! phew
```

몇 가지 유의사항이 있다. 

**Cache forward pass variables.** 역방향 패스를 계산시 정방향 연산에서 계산 된 변수 중 일부를 재사용하는 것이 좋다. 실제로 이러한 변수를 캐시하고 역전파 중에 사용할 수 있도록 코드를 구조화한다. 이 작업이 너무 어려울 경우 다시 계산 할 수 있지만, 쓸데없이 연산량을 낭비하게 된다. 

**Gradients add up at forks.** 위 정방향 연산에서 변수 x, y는 여러 연산자의 입력으로 사용되었다. 이 경우 역정파를 수행 할 때 변수에 그라데이션을 축적하는 += 를 사용해야 하며 =을 사용해서 기존 그라디언트에  덮어 쓰지 않도록 주의 해야한다. 이것은 미적준의 다중 변수 체인 룰(multivariable chain rule) 을 따르며 변수가 회로의 다른 부분으로 분기하는 경우, 역전파시 분기에서 전달되는 그라디언트를 더해 줘야한다. 

##### Patterns in backward flow

흥미로운 사실은 대부분의 경우 역방향으로 전달되는 그라디언트를 직관적으로 해석 할 수 있다는 점이다. 예를 들어, 뉴럴 네트워크에서 가장 일반적으로 사용되는 세 개의 게이트 ( add, mul, max )는 모두 역전파 중에 어떻게 동작하는지 간단하게 해석을 할 수 있다. 아래 예제 회로를 보자.

![](http://cfile23.uf.tistory.com/image/99115A3C5A687C2A0DD1ED "backward" "width:600px;float:center;padding-left:10px;")

입력에 대한 그라디언트를 계산하기 위해 역전파 중에 수행하는 연산을 직관적으로 보여주는 예제 회로. 합 연산은 모든 입력에 균등하게 그라디언트를 분배한다. MAX 연산은 그라디언트를 높은 입력값이 오는 쪽으로 보낸다. 곱연산 게이트는 입력을 가져온다음 이를 스왑한뒤 그라디언트와 곱해 전달한다. 

위의 다이어그램을 예로 들면, 다시 살펴보자 :

**합연산 게이트:** 항상 출력에 대한 그라이언트를 값에 상관없이 모든 입력에 똑같이 분배한다. 이는 합 연산에 대한 로컬 그래디언트가 단순히 +1.0이므로 모든 입력의 그래디언트에 x1.0으로 곱해질 것이기 때문에 출력의 그래디언트는 변경되지 않은 상태로 유지된다. 위의 예제 회로에서, + 게이트는 2.00의 그라디언트를 두 입력에 모두 동일하게 그리고 변경없이 전달했다.

**MAX 게이트:** 그라디언트를 변경하지 않고 모든 입력에 분산시킨 추가 게이트와 달리 최대 게이트는 그라디언트 (변경되지 않음)를 해당 입력 중 하나(최대 값을 갖는 입력)에 분배한다. MAX 게이트에 대한 로컬 그래디언트는 가장 큰 값에 대해 1.0이고 다른 모든 값에 대해 0.0이기 때문이다. 위의 예제 회로에서 max 연산은 그래디언트2.00을 w 보다 높은 값을 갖는 z 입력으로 전달하고 w의 그래디언트는 0을 유지한다.

**곱연산 게이트:** 이 연산자에 대한 해석은 앞의 둘 보다는 조금 더 어렵다. 해당 로컬 그래디언트는 스왑된 입력 값과 같으며, 체인 룰에 따라 출력에 대한 그래디언트가 곱해진다. 위의 예에서 x 의 그래디언트 는 -8.00이고 이는 -4.00 x 2.00을 통해 계산 된 값이다. 

**비 직관적인 경우와 그 결과.** 곱연산의 두 입력이 하나는 매우 크고, 다른 하나는 매우 작은 경우에 게이트는 약간 비직관적으로 동작한다. 상대적으로 큰 그라디언트가 작은 게이트 입력에 할당되며, 작은 그라디언트가 큰 입력에 할당된다. 선형 분류기에서 웨이트는 입력과 내적($w^Tx_i$, 곱)되며, 이는 데이터의 크기가 웨이트의 크기에 영향을 미친다는 것을 의미한다. 예를 들어, 만약 모든 입력에 1000을 곱하는 경우 웨이트에 대한 그라디언트는 1000배 커지게 되며, 따라서 learning late을 줄여 그라디언트 크기를 조정해야한다. 이런 이유로 데이터 전처리는 매우 중요하며, 때로는 미묘한 부분에서  문제가 되기도 한다. 그래디언트가 전달되는 방식을 직관적으로 이해하면 이러한 경우 중 일부를 디버그하는 데 도움이 된다.

##### Gradients for vectorized operations

위에서는 단일 변수에 대해 다뤘지만, 행렬 및 벡터 연산으로 확장할 수 있다. 그러나 행렬, 벡터의 차원이나, transpose 연산에 주의 해야한다. 

**Matrix-Matrix multiply gradient.** 아마도 가장 복잡한 연산은 행렬 - 행렬 곱셈(모든 행렬-벡터 및 벡터-벡터 곱셈을 일반화하는)연산이다.

``` python
# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)
```

*팁 :* 차원 분석을 사용하라!   dW와 dX 의 표현식을 기억하지 않아도 된다. 그 차원에 따라 쉽게 유도 할 수 있기 때문이다. 예를 들어, X와 dD의 곱으로 구해지는 웨이트의 그라디언트 dW는  W와 동일한 크기를 가져야한다. 이를 만족하는 한 가지 방법이 항상 존재한다. 예를 들어, X크기는 [10 X 3], dD 의 크기는 [5 × 3]이고 dW및 W가 [5 × 10] 일때, 이를 만족하는 유일한 방법은 위에 적은 바와 같이 dD.dot(X.T) 뿐이다.
 
**Work with small, explicit examples.** 처음에는 벡터화 된 표현식에 대한 그래디언트 업데이트를 유도하기 어려울 수 있다. 간단한 벡터화 된 예제에 대한 그라디언트를 종이에 유도 한 다음,  점차 일반적인 형식으로 벡터화하는 것을 추천한다.

Erik Learned-Miller는 행렬/벡터 편미분을 이해하는데 도움이 될 수 있는 문서를 작성했다. [여기](http://cs231n.stanford.edu/vecDerivs.pdf)에서 볼 수 있다. 

##### Summary

- 그라디언트가 의미하는 바를 이해하고, 회로 내에서 어떻게 역방향으로 전파되는지, 그리고 최종 출력을 더 크게 만들기 위해 회로의 어느 부분이 증가 또는 감소해야하는지에 대한 직관을 터득했다. 
- 역전파(backpropagation)의 실제 구현을 위해 단계적 계산의 중요성에 대해 논의했다. 함수를 로컬 그라디언트를 구하기 쉬운 모듈들로 분해한 다름 체인 룰을 통해 이를 연결했다. 결정적으로, 종이에 복잡한 수학실을 서가며 함수에 대한 미분식을 유도할 필요는 없다. 왜냐하면 입력 변수의 그래디언트에 대한 명시적인 미분식이 필요 없기 때문이다. 함수를 스테이지(모듈)로 분해하여 각 스테이지를 독립적으로 미분 할 수 있다. (여기서 스테이지는 행렬 벡터 곱연산, Max 연산 또는 합연산 등). 그런 다음 한 번에 한 스테이지씩 역전파를 진행한다. 

다음 섹션에서는 신경망을 정의 할 것이고, 역전파를 통해 손실 함수에 따른 출력에 대한 그라디언트를 신경망의 연결을 따라 효율적으로 계산할 것이다.  즉, 이제 우리는 신경망을 훈련할 준비가 되었다. 이 수업에서 개념적으로 이해하기 가장 어려운 부분이 다음 섹션에 있다! 그리고 ConvNets으로 한 발짝 내딛을 것이다.

##### References

- [Automatic differentiation in machine learning: a survey](http://arxiv.org/abs/1502.05767)