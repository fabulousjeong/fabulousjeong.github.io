---
title: 24 Maximum Expected Utility
category: ProbabilisticGraphicalModels
excerpt: |
  지난 강의 동안 probabilistic graphical models을 통해 조건부 확률 분포를 어떻게 구하는지 또는 network를 어떻게 구성하는지 다뤘습니다. 하지만 우리가 정말 하고 싶은 것은 확률 분포를 통해 의사결정(decision making)을 하는 것입니다.


feature_text: |
  ## [Coursera] Probabilistic Graphical Models
  Daphne Koller의 Probabilistic Graphical Models 강의 정리

  ref: [https://www.coursera.org/learn/probabilistic-graphical-models/home](https://www.coursera.org/learn/probabilistic-graphical-models/home "coursera")

feature_image: "https://picsum.photos/2560/600/?image=798"
image: "https://picsum.photos/2560/600/?image=798"
comment: true
---


##### decision making

![](http://cfile2.uf.tistory.com/image/276C044758BD4C1309F304 "simple decision making" "width:600px;float:center;padding-left:10px;")

지난 강의 동안 probabilistic graphical models을 통해 조건부 확률 분포를 어떻게 구하는지 또는 network를 어떻게 구성하는지 다뤘습니다. 하지만 우리가 정말 하고 싶은 것은 확률 분포를 통해 의사결정(decision making)을 하는 것입니다. 가령 의사가 환자를 진료 할때는 환자가 어떤 병에 결렸는지 파악하는 것 만으로는 충분하지 않습니다. 어떤 치료를 해야하는지 결정해야 합니다. 이러한 결정을 내리기 위해 probabilistic graphical models을 어떻게 사용 할 수 있을까요? 이를 위해 maximum expected utility(최대 기대 효용, MEU)라는 방법을 사용 할 수 있습니다. 사실 이 방법은 probabilistic graphical models이 본격적으로 나오기 전부터 활용되어 왔습니다. 이제 위와 같은 셋들이 주어집니다. 이는 순전히 확률 모델만 있는 것이 아니라 다른 요소들도 있습니다. 우리가 선택할 수 있는 Action A가 있습니다. 환자에게 처방 할 "약"이 이에 해당 될 것입니다. 이러한 액션은 State(상태)에 영향을 줍니다. 혹은 안 줄수도 있을 것입니다. 이것은 확률이며 $P(X|A)$로 표현 할 수 있습니다. 마지막으로 선호도를 정의 할 수 있습니다. 이러한 선호도 함수를 통해 행동 A를 취할때 얼마나 높은 점수를 주는지에 대한 수치를 얻을 수 있습니다. 수치적 유틸리티 함수를 통해 이 행동에 대한 수치적 선호도 값을 구할 수 있습니다. 

![](http://cfile29.uf.tistory.com/image/261E464758BD4C14042407 "expected utility" "width:600px;float:center;padding-left:10px;")

기대 효용 함수를 통해 액션 A가 주어진 결정에 대한 값을 구할 수 있습니다. 그리고 주어진 A에대한 x의 조건부 확률도 있습니다. 이를 곱한다음, 위의 식과 같이 모든 x에 대해 더하면, 액션 A에 대한 전반적인 행복도(기대값)을 구할 수 있습니다. 이제 우리가 하고 싶은 것은 이러한 행복도(기대값)이 최대가 되는 a를 구하는 것 입니다.

![](http://cfile3.uf.tistory.com/image/2336F74758BD4C1402CE01 "influence diagram" "width:600px;float:center;padding-left:10px;")

influence diagram을 이용하여 행복도(기대값)이 최대가 되는 a를 구할 수 있습니다. influence diagram은 베이지안 네트워크에 두 부분을 추가하여 만들 수 있습니다. 일반적으로 influence diagram에는 타원으로 표현 된 랜덤 변수가 있습니다. 이러한 랜덤 변수는 State X의 부분 집합입니다. 그리고 여기는 두 다른 종류의 노드가 추가로 있습니다. 하나는 직사각형으로 표현된 액션 변수 입니다. 액션 변수는 값을 선택하지 않기 때문에 확률 분포가 주어 지지 않습니다. 그리고 마름모꼴로 표현된 유틸리티 함수가 있습니다. 위 예제는 사실 우리가 구성할 수 있는 가장 간단한 형태의 influence diagram입니다. 우리가 대학을 졸업한 상황을 가정해 봅시다. 우리가 선택할 수 있는 행동(Action)으로 벤처회사를 설립할 수도($f^1$) 혹은 그냥 취업 할 수($f^0$)도 있습니다. 또한 무작위 변수로 현재 시장 상태($m^1, m^2, m^3$)가 있습니다. 유틸리티 함수에는 이러한 시장 상태와 회사 설립 여부에 따른 선호도 값에 대한 표가 있습니다. 이제 앞에서 다룬 식을 이용하여 각 행동에 대한 기대 값을 구할 수 있습니다. 위의 식과 같이 $f^0$의 경우 0의 기대 값을 가집니다. 그리고 $f^1$의 경우 2의 기대 값을 가집니다. 따라서 이러한 경우 회사를 설립($f^1$)하는 것이 더 나은 선택임을 알 수 있습니다. 

![](http://cfile30.uf.tistory.com/image/212E794758BD4C151BD22D "complex influence diagram" "width:600px;float:center;padding-left:10px;")

조금 더 복잡한 네트워크를 보겠습니다. 위 네트워크는 학생 성적 네트워크 예제의 조금 더 복잡한 버전입니다. 위 모델에서 학생의 지능, 과목의 난이도, 공부의 여부는 과목의 성적에 영향을 미칩니다. 그리고 과목의 성적은 추천서에 영향을 줍니다. 성적과 추천서는 학생의 직업에 영향을 줍니다. 이제 우리는 공부를 할 것인지 안 할 것인지를 결정하려 합니다. 이를 위해 세 유틸리티 함수($V_G, V_S, V_Q$)를 추가 하였습니다. $V_G$는 성적에 대한 행복도 입니다. 예를 들어 A를 받으면 큰 값을 가질 것이며 C를 받으면 낮은 값을 가질 것입니다. $V_S$는 좋은 직업에 대한 행복도를 나타내며, $V_Q$는 삶의 질에 대한 행복도를 나타냅니다. 우리가 공부를 하지 않고 영화를 보러 가거나 데이트를 한다면 공부하는 것보다 더 큰 행복도를 줄 수 있을 것입니다. $V_Q$는 과목의 난이도에 따라 그 값이 달라지는데 가령 쉬운 과목이라면 공부하는게 재미있어서 더 큰 행복도를 줄 수도 있을 것입니다. 모든 유틸리티 함수의 합을 통해 우리는 전체 유틸리티 함수를 구할 수 있습니다. 이렇게 해서 구한 전체 유틸리티 함수를 통해 기대 값을 얻을 수 있지만 각 함수를 분해해서 기대 값을 얻는 것이 더 좋습니다. 인수가 기하 급수의 형태기 때문에 여러 함수를 합치는 것은 좋지 않습니다. 예를 들어 위의 함수를 통합한다면 각 변수가 2진 값이라 하더라도 4개 값이 이용되므로 $2^4=16$ 개의 파라미터가 필요합니다. 따라서 유틸리티 함수를 분해하는 것이 계산을 더 간단하게 만듭니다. 

![](http://cfile1.uf.tistory.com/image/240A024758BD4C1606621A "information edges" "width:600px;float:center;padding-left:10px;")

앞에서는 Action에 영향을 주는 것이 없으며 따라서 독립적인 요인이라 가정했습니다. 이제 이 개념을 확장하여 엑션에 영향을 주는 정보가 있다고 가정하겠습니다. 위의 예는 앞서 다룬 창업을 확장한 것입니다. 이제 시장에 대해 설문 조사를 한다고 가정해 봅니다. 사실 이 설문은 실제 시장의 상태를 정확히 반영하지 못합니다. 이 역시 CPD로 위 표와 같이 표현 됩니다. 요점은 회사를 설립하는 액션(Found)가 설문의 영향을 받는 것 입니다. 위와 같이 설문(S)이 주어졌을 때 회사를 설립(F)할 확률($P(F\|S)$)을 정의 할 수 있습니다. 따라서 이제 위 모델에서 액션은 독립적인 것이 아니라 설문의 결과에 따라 다른 결정을 내립니다. 이러한 정의를 의사결정규칙(Decision Rule, $\delta$)이라하며 조건부 확률($P(A\|Parents(A))$)로 표현됩니다. 위 예에서는 세가지 상황($s^0, s^1, s^2$)에 따라 어떤 액션($f^0, f^1$)이 나올지에 대한 확률이 정의 될 것입니다. 우리는 최적의 선택을 하기위한 액션을 찾길 원하기 때문에, 사실 위 조건부 확률을 표현할 필요는 없습니다. 하지만 뒤에서 살펴보겠지만 CPD로 생각하는 것이 유용합니다. 

![](http://cfile21.uf.tistory.com/image/25109D4758BD4C1736641D "utility with information" "width:600px;float:center;padding-left:10px;")

위의 식과 같이 기대 값을 계산 할 수 있습니다. Decision Rule이 조건부 확률이므로 이를 포함한, 네트워크의 전체 확률 분포를 $P_{\delta_A}$로 표현 할 수 있습니다. 기대 값은 확률 분포와 유틸리티 함수의 곱입니다. 모든 a와 x에 대한 기대 값은 더하면 전체 기대 값이 되고 이것을 최대로 만드는 $\delta_A$를 찾는 것이 목표힙니다. 그렇다면 위 식이 최대값이 되게 하는 $\delta_A$를 어떻게 찾을 수 있을까요?   

![](http://cfile21.uf.tistory.com/image/2579C84758BD4C18205B00 "MEU" "width:600px;float:center;padding-left:10px;")

먼저 간단한 앞의 예제를 보겠습니다. 위 전체 네트워크에 대한 CPD인 $P(M), P(S\|M)$을 써줍니다. 그리고 Action의 CPD인 $\delta_F(F\|S)$와 유틸리티 함수가 있습니다. 이를 써보면 위 식과 같습니다. 마치 여러 확률 분포의 곱처럼 보입니다. 이는 앞에서 본 Factor의 곱으로 전체 확률 분포를 구했던 것과 유사합니다. 다만 U는 확률 분포가 아닌 수치적인 값을 가지고 있습니다. 하지만 이 역시 F와 M에 영향을 받습니다. 이제 액션을 결정하는데 사용하지 않는 변수들을 묶습니다. 위 예에서는 M으로 묶을 수 있습니다. M으로 묶인 결과를 $\mu(F,S)$라 하겠습니다. $\mu$로 표현한 이유는 유틸리티 함수 U를 포함하기 때문입니다.   

![](http://cfile30.uf.tistory.com/image/212F5D4E58BD4C191B8C9D "MEU" "width:600px;float:center;padding-left:10px;")

앞선 확률분포표를 이용하여 $\delta_F(F\|S)\mu(F,S)$를 구하면 위와 같이 나옵니다. 예를 들어 $s^0, f^1$의 경우 $P(m^0)P(s^0\|m^0)U(m^0,f^1)+P(m^1)P(s^0\|m^1)U(m^1,f^1)+P(m^2)P(s^0\|m^2)U(m^2,f^1) = -1.25$가 나옵니다. 다른 경우도 같은 방법으로 구할 수 있습니다. 단 유틸리티 함수 U에서 $f^0$의 경우 0이므로 $\delta_F(F\|S)\mu(F,S)$에서도 모든 행이 0값을 가집니다. 이제 위 식에서 최대 값을 가지려면 조사 결과가 $s^0$인 경우 $f^0$를 선택하고, $s^1$인 경우 $f^1$, $s^2$인 경우 $f^1$을 선택하는 것입니다. 그리고 그 결과는 0+1.15+2.1인 3.25입니다. 이는 지난 모델에서 시장 조사 없이 무조건 $f^1$을 고른 값인 2에 비해 1.25나 더 증가한 값입니다. 설문을 통해 시장이 좋지 않은 경우 창업을 하지 않기 때문에 그 만큼 기대 값이 더 올라 간 것을 볼 수 있습니다.

![](http://cfile3.uf.tistory.com/image/225F4D4E58BD4C1A03DD02 "Generally" "width:600px;float:center;padding-left:10px;")

앞선 과정을 일반화 해보겠습니다. 우선 기대 값을 공통 확률 분포와 유틸리티 함수로 표현했습니다. 그 다음 공통 확률 분포를 네트워크의 CPD들과 $\delta_A$의 곱으로 분해 했습니다. 그 다음 우리가 알고 있는 유틸리티 함수와 네트워크의 CPD들을 $\mu$로 다시 묶었습니다. 이제 우리는 앞에서와 같이 각 조건 Z마다 값이 최대가 되게 하는 a를 선택하는 일을 수행하면 됩니다. 이것을 식으로 표현하면 위와 같습니다. 각 주어진 z 조건에서 최대가 되는 경우 a를 선택하는 것은, 그 경우 $\delta_A$ 값을 1로 설정하고, 그 외에는 0으로 하는 것으로 표현 할 수 있습니다. 그렇기 때문에 $\delta$ 함수로 표현 됩니다. 즉 CPD지만, 선택된 액션 값에 대해서만 1을 가지고 나머지는 0의 값을 가지는 확률 분포입니다. 

![](http://cfile30.uf.tistory.com/image/210A7E4E58BD4C1B2CD39D "MEU Summary" "width:600px;float:center;padding-left:10px;")

따라서 알고리즘이 최대 기대값을 가지는 액션 A를 찾아 의사결정하는 과정을 요약하면 다음과 같습니다. 우선 임의의 확률 분포로 표현되는 A를  랜덤변수로 취급합니다. 이제 A를 포함한 전체 네트워크는 factor의 곱으로 표현 됩니다. A와 A의 부모 노드인 Z를 제외한 나머지를 $\mu$로 묶습니다. 각 조건 Z에 대해 $\mu(A,z)$가 최대가 되는 A의 액션(a)를 선택 합니다.  

![](http://cfile25.uf.tistory.com/image/23475B4E58BD4C1C0582E4 "Decision Making" "width:600px;float:center;padding-left:10px;")

MEU(maximum expected utility)는 불확실성이 있는 상태에서도 정확한 의사 결정에 대한 기준을 제공합니다. PGM은 여러 확률 분포를 가지는 변수들, 액션들, 유틸리티 함수들로 표현됩니다. PGM을 통해 식을 세우고, 앞서 다룬 변수 제거법(variable elimination, VE)를 사용하여 각 조건에 대한 최적의 액션을 선택할 수 있습니다. 또한 여기서 다루지는 않았지만 여러 유틸리티 함수를 가지는 경우나, 여러 액션을 가지는 경우에 대해서도 위 방법을 확장하여 적용 할 수 있습니다.  



