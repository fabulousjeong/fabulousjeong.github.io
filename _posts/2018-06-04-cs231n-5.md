---
title: cs231n 번역 5 Neural Networks Part 1 Setting up the Architecture
excerpt: |
 인간의 뇌과 비교하지 않아도, 신경망에 대해 소개할 수 있다. 이전 선형 분류 섹션에서 공식 $s=Wx$를 사용하여 주어진 이미지의 여러 카테고리에 대한 스코어를 구했다. 

feature_text: |
  ## Stanford CS class CS231n: 

  Convolutional Neural Networks for Visual Recognition 강의 번역

  ref: [http://cs231n.github.io/](http://cs231n.github.io/ "cs231n")

feature_image: "https://picsum.photos/2560/600/?image=849"
image: "https://picsum.photos/2560/600/?image=849"
comment: true
---


##### Quick intro
인간의 뇌과 비교하지 않아도, 신경망에 대해 소개할 수 있다. 이전 선형 분류 섹션에서 공식 $s=Wx$를 사용하여 주어진 이미지의 여러 카테고리에 대한 스코어를 구했다. 여기서 $W$는 행렬이며, $x$는 이미지의 모든 픽셀에 대한 정보를 담고 있는 열벡터이다. CIFAR-10의 경우 $x$는 [3072x1] 열벡터이며, $W$는 [10x3072] 행렬이다. 따라서 출력 벡터는 10개 클래스에 대한 스코어를 담고 있는 [10x1] 벡터가 된다.  

신경망의 다른 예로 다음과 같은 공식 $s = W_2 \max(0, W_1 x)$도 있을 수 있다. 여기서 $W_1$의 한 예로 이미지를 크기 100을 가지는 중간 벡터로 바꾸는 [100x3072] 행렬이 될 수 있다. 함수 $max(0,-)$는 입력에 따라 비선형성을 보인다.  비선형성을 만드는 몇 가지 방법이 있다. 가장 일반적인 방법은 위와 같이 0이하의 값에 대해 threshold를 부여 하는 것이다. 끝으로 $W_2$는 [10x100]행렬이며 중간 벡터로부터 10개 클래스의 스코어를 얻는다. 비선형성은 계산적으로 중요하다. 만약 비선형 항이 없다면 두 행렬을 단일 행렬로 축소 할 수 있으므로 클래스 스코어는 다시 입력의 선형 함수가된다. 이런 비선형성에서 흔들림(wiggle)이 생긴다. 파리미터 $W_1, W_2$는  stochastic gradient descent에 의해 학습되며, 체인 룰과 역전파 계산으로 그라디언트를 유도한다. 

다음과 같이 레이어가 세 개인 경우 $s = W_3 \max(0, W_2 \max(0, W_1 x))$를 생각해 볼 수 있다. 여기 파라미터 $W_1, W_2, W_3$는 모두 학습된다. 중간 히든 벡터의 크기는 네트워크의 하이퍼 파라미터며, 아래에서 어떤 값으로 설정할지 살펴보겠다. 뉴런/네트워크 관점에서 이러한 식을 해석하는 법을 살펴보겠다.  

##### Modeling one neuron
Neural Networks는 원래 생물학적 신경계를 모델링하기 위해 도입되었지만, 그 이후로는 공학적인 머신러닝 문제를 해결하는데 사용되어 좋은 결과를 얻었다. 그럼에도 불구하고, 처음 영감을 준 생물학적 시스템에 대한 매우 간략하고 높은 수준의 설명으로 시작하겠다.

##### Biological motivation and connections

뇌의 기본적인 계산 유닛은 **뉴런(Neuron)**이다. 약 860억개의 뉴런이 인간의 신경계를 구성하고 있으며 약 $10^{14} ~ 10^{15}$ 개의 **(synapses)**와 연결되어 있다. 아래 다이어그램은 생물학적 뉴런(왼쪽)과 일반적인 수학적 모델(오른쪽)을 보여준다. 각 뉴런은 수상 돌기(dendrites)들로부터 입력 신호를 받아 그 (단일) 축색 돌기(axon)를 따라 출력 신호를 생성한다. 축색 돌기는 갈라져 시냅스를 통해 다른 뉴런의 수상 돌기에 연결된다. 뉴런의 수학적 모델에서, 축색 돌기을 따라 이동하는 신호 (예 : $x_0$)는 시냅스의 강도($w_0$)에 기반하여 다른 뉴런의 수상돌기와 곱($w_0 x_0$)을 통해 상호작용한다. 시냅스의 강도는 학습가능하며, 이를 통해 뉴런이 다른 뉴런에 미치는 영향(양수: 흥분성, 음수: 억제성)을 제어한다. 기본 모델에서, 수상 돌기는 세포 중앙으로 신호를 전달하여 모든 신호가 합쳐진다. 최종 합계가 특정 임계 값을 초과하면 뉴런은 발사(fire) 할 수 있으며 따라서 축색 돌기를 따라 스파이크를 전달한다. 수학적 모델에서 스파이크가 발사되는 정확한 타이밍은 중요하지 않으며, 특정 발사 주기에 따라 정보를 전달한다고 가정한다. 이를 바탕으로 뉴런의 발사율 를 **활성화 함수($f$)** 통해 모델링하며, 이는 축색 돌기를 따라 흐르는 스파이크의 빈도를 나타낸다. 전통적으로, 활성화 함수로 **시그모이드 함수 ($\sigma$)**를 주로 사용하였다. 이는 시그모이드 함수가 주어진 입력을 0~1사이 범위로 축소시키는 특성을 가지고 있기 때문이다. 뒷 부분에서 활성화 함수에 대한 설명을 이어 나가겠다. 

![](http://cs231n.github.io/assets/nn1/neuron.png "neuron" "width:600px;float:center;padding-left:10px;")

![](http://cs231n.github.io/assets/nn1/neuron_model.jpeg "neuron_model" "width:600px;float:center;padding-left:10px;")

생물학적 뉴런 (위)과 수학적 모델 (아래)의 그림

```python
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """ assume inputs and weights are 1-D numpy arrays and bias is a number """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
    return firing_rate
```

즉, 각 뉴런은 입력과 그 웨이트를 내적한 다음, 바이어스를 더하고, 비선형성(또는 활성화 함수)을 적용한다. 이 경우 시그모이드($\sigma(x) = 1/(1+e^{-x})$) 함수를 사용하였다. 이 절의 끝에서 다양한 활성화 함수에 대해 설명하겠다.

**Coarse model.** 위 생물학적 뉴런 모델은 매우 기본적이다는 것을 다시 한 번 강조한다. 예를 들어, 실제 여러가지 유형의 뉴런이 있으며, 각기 다른 특성을 지닌다. 그리고 생물학적 뉴런의 수상돌기는 복잡한 비선형 계산을 수행한다. 시냅스는 단순한 웨이트가 아니라 복잡한 비선형 동적 시스템이다. 많은 시스템에서 출력 스파이크의 정확한 타이밍이 중요하므로 특정 주기로 근사화 한 가정이 유지되지 않을 수 있다. 이 사항들과 다른 많은 단순화 때문에 신경망과 실제 두뇌에 대한 비교는 신경 과학 배경을 가진 사람의 짜증을 유발한다. 다음 리뷰(pdf)를 보거나, 관심이 있다면 최근 리뷰를 참고하길 권한다. 

**Single neuron as a linear classifier**

Neuron의 순방향 계산 모델에 대한 수학적 형태는 익숙 할 것이다. 선형 분류기에서 보았듯이, 뉴런은 입력의 특정 선형 영역에 대해 "좋음"(거의 활성화) 또는 "싫음"(0 근처의 활성화)을 표현한다. 그러므로, 뉴런의 출력에 대한 적절한 손실 함수를 도입하며, 단일 뉴런을 선형 분류기로 바꿀 수 있다 :

**Binary Softmax classifier.** 한 개 클래스에 대한 확률 $P(y_i = 1 \mid x_i; w)$를 다음과 같이 $\sigma(\sum_iw_ix_i + b)$로 표현 할 수 있다. 여기서 두 클래스에 대한 확률의 합이 1이므로, 다른 한 클래스에 대한 확률은 다음과 같다. $P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w)$ 이 해석을 통해, 선형 분류 섹션에서 보았 듯이 교차 엔트로피 로스를 공식화 할 수 있으며, 최적화 과정을 통해 이진 Softmax 분류기 (로지스틱 회귀 라고도 함)를 이끌어 낼 수 있다. 시그모이드 함수는 출력을 0-1 사이로 제한하기 때문에 이 분류 기준의 예측은 뉴런의 출력이 0.5보다 큰지를 기반으로 한다.

**Binary SVM classifier.** 다른 방법으로, 뉴런의 출력에 최대 마진 힌지 로스를 덧붙여 바이너리 SVM 분류기로 훈련시킬 수 있다.

**Regularization interpretation.** SVM/Softmax 경우에서의 정규화 로스는 생물학적 관점에서 점진적인 망각으로 해석 할 수 있다. 왜냐하면 모든 시냅스 가중치($w$)는 매개 변수를 업데이트 한 후에 0을 향해 줄어 들기 때문이다.

*뉴런을 사용하여 이진 분류기 (예 : 이진 Softmax 또는 이진 SVM 분류기)를 구현할 수 있다.*

##### Commonly used activation functions

모든 활성화 함수 (또는 비선형성: non-linearity )는 한 값을 받아 취하여 특정 수학 연산을 수행한다. 실제로 사용되는 몇가지 활성화 함수를 살펴본다. 

![](http://cs231n.github.io/assets/nn1/sigmoid.jpeg "sigmoid" "width:600px;float:center;padding-left:10px;")

 Sigmoid 비선형성: 실수값을 [0,1] 사이의 범위로 축소한다. 
 
![](http://cs231n.github.io/assets/nn1/tanh.jpeg "tanh" "width:600px;float:center;padding-left:10px;")

tanh 비선형성은 실수를 [-1,1] 사이의 범위로 축소한다. 

**Sigmoid.** sigmoid 함수는 위 그림의 왼쪽이며 그 수학적 표현은 다음과 같다. $\sigma(x) = 1 / (1 + e^{-x})$ 앞에서 언급했듯이, 이 함수는 실수 값을 입력 받아 0과 1 사이의 범위로 "축소(squashes)"한다. 특히 매우 큰 음수는 0이 되며 매우 큰 양수는 1이 된다. 시그모이드 함수는 뉴런의 발사를 표현하기 좋기 때문에, 전통적으로 자주 사용되었다: 뉴런 신호를 발사하는 경우 1 하지 않는 경우 0의 값을 가짐. 하지만 시그모이드 비선형성은 최근에는 거의 사용되지 않는다.  시그모이드 함수는 두 가지 주요 단점이 있다.

- Sigmoids는 포화되며, 그라디언트 값이 없어 질 수 있다. 시그모이드 뉴런의 매우 좋지 않은 특성은 뉴런의 활성화 값이 포화되어 0 또는 1의 근처에 있는 경우, 그 기울기가 거의 0이 된다는 것이다. 역전파 과정에서 이 (로컬)그래디언트가 전체 로스값에 대한 이 게이트 출력의 그래디언트에 "곱"해진다는 것을 떠올려 보자. 따라서 로컬 그래디언트가 매우 작으면 전체 그래디언트 역시 작아지므로, 뉴런을 통해 해당 웨이트로 그리고 재귀적으로 해당 데이터로 흐르는 신호가 거의 없어 진다. 또한, 포화를 방지하기 위해 시그모이드 뉴런의 웨이트 파라미터를 초기화 할 때 각별한 주의를 기울여야한다. 예를 들어 초기 가중치가 너무 크거나 작으면 대부분의 뉴런이 0이나 1의 값을 가져, 포화되므로 네트워크의 학습이 매우 더디게 진행 될 것이다. 
- 시그모이드 함수의 출력값의 중심은 0이 아니다. 이것은 Neural Network에서 다음 레이어의 입력 뉴런이 곧 zero-centered가 아닌 데이터를 받을 것이므로 바람직하지 않다. 왜냐하면 뉴런에 들어오는 데이터가 항상 양의 값을 가지기 때문에 그라디언트 디센트(gradient descent) 과정의 동역학에 영향을 미친다. 입력 $x$는 항상 양수이므로 역전파 과정에서 $f = w^Tx + b$의 그래디언트의 원소는 항상 모두 양수값을 가지거나 아니면 모두 음수 값을 가질 것이다. 이는 웨이트에 대한 그래디언트 업데이트에서 지그재그 형태로 수렴하는 좋지 않은 동역학을 초래할 수 있습니다. 하지만 배치(batch) 단위로 학습하면 그라디언트가 여러 부호를 가질 수 있으므로 이 문제를 완화 할 수 있다. 이것은 다소 불편함을 초래하지만, 위 첫번째 문제에 비하면 심각한 일은 아니다. 

**Tanh.** tanh 함수는 위의 그림의 오른쪽과 같이 실수 값을 범위 [-1, 1]로 축소한다. 시그모이드 뉴런과 마찬가지로 양끝에서 값이 포화되지만 시그모이드와는 달리 출력의 중심은 0이다. 따라서, 실제로는 tanh 비선형성은. 항상 Sigmoid 비선형성보다 선호된다. 또한 tanh 뉴런은 Sigmoid 뉴런의 단순한 확장으로 표현 할 수 있다. $\tanh(x) = 2 \sigma(2x) -1$

![](http://cs231n.github.io/assets/nn1/relu.jpeg "relu" "width:600px;float:center;padding-left:10px;")

ReLU 활성화 함수 $x$가 0보다 작은 경우 0의 값을, 큰 경우 1의 기울기를 가진다.  
 
![](http://cs231n.github.io/assets/nn1/alexplot.jpeg "alexplot" "width:600px;float:center;padding-left:10px;")

ReLU의 경우 tanh에 비해 수렴속도가 거의 6 배 향상되었음을 볼 수 있다.

**ReLU.** ReLU는(Rectified Linear Unit, $f(x) = \max(0, x)$)은 지난 몇 년 동안 매우 인기있는 활성화 함수로 사용되었다. 위 식과 같이 이 함수는 단순히 0의 임계 값에 의해 처리된다. (왼쪽 위의 이미지 참조). ReLU를 사용하는 데는 몇 가지 장단점이 있다. 

- (+) Sigmoid / tanh 함수에 비해 확률적 그래디언트 강하의 수렴 속도가 크다. 이것은 함수의 형태가 선형적이며 포화되지 않기 때문이다. 
- (+) 복잡한 연산 (지수 등)을 포함하는 tanh / sigmoid 뉴런과 비교할 때, ReLU는 단순히 0으로 thresholding하는 것 만으로 구현할 수 있다.
- (-) 불행하게도, ReLU 유닛은 학습 중에 취약점이 있고, 다시는 동작하지 않을 수도 있다. 예를 들어, ReLU 뉴런을 통해 흐르는 그라디언트로 인해 뉴런이 모든 데이터 포인트에서 0의 값이 출력 되게 가중치가 업데이트 될 수 있다. 이 그래디언트는 0의 값을 가지며 파라미터는 영원히 변하지 않는다.  즉, ReLU 유닛은 데이터 매니 폴드에 빠질 수 있으므로, 훈련 중에 다시는 동작하지 않을 수 있다. 예를 들어, 학습 속도가 너무 높게 설정된 경우 네트워크의 40 % 정도가 동작하지 않았다. 적절한 학습 속도 설정으로 이 문제를 해결 할 수 있다. 

**Leaky ReLU.** Leaky ReLU는 "ReLU"의 학습 중 비동작 문제를 해결하려는 시도 중 하나다. $x$가 0보다 작을 때 0의 값을 가지는 대신 다음과 같이 매우 작은(0.01) 기울기를 가진다. $f(x) = \mathbb{1}(x < 0) (\alpha x) + \mathbb{1}(x>=0) (x)$ 여기서 $\alpha$는 매우 작은 상수 값이다. 음수 영역에서의 기울기는 Kaiming He et al., 2015의 PReLU 뉴런에서 볼 수 있듯이 파라미터로 볼 수 있다 . 하지만 이러한 형태의 활성화 함수가 꼭 성공적인 결과를 가져오지는 않으며, 전반적이 장점에 대해서는 현재 불분명하다. 

**Maxout.** 상대적으로 많이 사용되는 다른 활성화 함수로는 maxout이 있다. 이 함수는 ReLU, Leaky ReLU 함수의 일반적인 표현이다. 그 형태는 다음과 같다. $\max(w_1^Tx+b_1, w_2^Tx + b_2)$ ReLU의 경우 $w_1, b_1$이 0인 위 함수의 특별한 케이스다. 따라서 Maxout 뉴런은 ReLU 유닛의 모든 장점(선형 동작, 포화 상태 없음)을 누리고 단점 (ReLU로 인한 비동작)이 없다. 그러나 ReLU 뉴런과는 달리 모든 단일 뉴런에 대한 파라미터의 수가 두 배로 증가하므로 파라미터수가 많아 지게 된다. 

이것으로 가장 일반적인 유형의 뉴런과 그 활성화 함수에 대한 논의를 마친다. 마직막으로 언급할 말은 근본적으로 이것이 잘못된 일이 아님에도 불구하고, 한 네트워크에서 서로 다른 유형의 뉴런을 혼합거나 일치시키는 일은 매우 드물다. 

**TLDR.** "어떤 종류의 뉴런을 사용해야 할까? " ReLU 함수를 사용하고, 학습 속도에 주의를 기울이며, 가능하다면 네트워크에서 동작하지 않는 유닛의 비율을 모니터한다. 만약 걱정된다면, Leaky ReLU나 Maxout을 시도해보자. Sigmoid를 사용하지 말자. tanh를 시도 해 볼 수 있지만, ReLU / Maxout보다 더 잘 작동 할 것은 기대하지 말자. 

#### Neural Network architectures

##### Layer-wise organization

**Neural Networks as neurons in graphs.** 뉴럴 네트워크는 비순환 그래프로 연결된 뉴런의 집합으로 모델링 된다. 즉, 일부 뉴런의 출력은 다른 뉴런의 입력이 될 수 있다. 사이클(순환)은 네트워크의 순방향 패스에서 무한 루프를 야기하기 때문에 허용되지 않는다. 뉴런 네트워크 모델은 뉴런 비정형적으로 연결되어 뭉쳐있기 보다는 뉴런의 별개 층으로 구성되는 경우가 많다. 전형적인 신경망 중 가장 일반적인 레이어 타입은 인접한 레이어의 원소들은 완전히 연결되어 있지만, 레이어 내부 뉴런 사이에는 연결이 없는 fully-connected layer(완전 연결레이어)다. 아래는 두 가지 완전 연결 레이어의 예를 보여준다. 

![](http://cs231n.github.io/assets/nn1/neural_net.jpeg "neural_net" "width:600px;float:center;padding-left:10px;")

 2-레이어 신경망, 세 뉴런으로 구성된 입력 및 4개 뉴런으로 구성된 하나의 히든레이어와 두 뉴런으로 구성된 출력 레이어 
 
![](http://cs231n.github.io/assets/nn1/neural_net2.jpeg "neural_net2" "width:600px;float:center;padding-left:10px;")

 3-레이어 신경망, 세 뉴런으로 구성된 입력 및 각각 4개 뉴런으로 구성된 두개의 히든레이어와 하나의 뉴런으로 구성된 출력 레이어. 두 경우 모두 뉴런 사이의 연결 (시냅스)이 인접 레이어 사이에서만 이루어지며 일어나지 않는다.
 
**Naming conventions.** N-레이어 신경망을 부를 때 입력 레이어는 계산하지 않는다. 따라서 입력이 출력에 직접 맵핑되는 단일 레이어 신경망은 히든 레이어가 없는 네트워크를 묘사한다. 그런 의미에서, 로지스틱 회귀 또는 SVM을 단일 레이어 신경망의 특별한 경우라고 말할 수 있다. 이 네트워크는 "인공 신경망 (ANN) " 또는 "다중 레이어 퍼셉트론 (MLP)" 이라고도 불린다. 대부분 뉴런 네트워크와 실제 두뇌 사이의 유사성을 좋아하지 않으며, 뉴런을 유닛으로 지칭하는 것을 선호한다 .

**Output layer.** 신경망의 다른 레이어들과 달리, 출력 레이어 뉴런은 일반적으로 활성화 함수가 없다. (또는 선형 항등(identity)함수가 있다고  볼 수 있다.). 이것은 최종 출력 레이어가 일반적으로 클래스 스코어(예 : 분류)를 의미하거나, 임의의 실수 값 또는 일종의 실수 값을 가지는 타겟(예 : 회귀) 나타내기 때문이다.  

**Sizing neural networks.** 일반적으로 신경망의 크기를 측정하는 데 사용하는 두 가지 측정 기준은 뉴런 개수 또는 더 일반적으로 매개 변수의 개수다. 위의 그림에 있는 두 개 네트워크를 통해 알아보자.  :

- Naming conventions. N-레이어 신경망을 부를 때 입력 레이어는 계산하지 않는다. 따라서 입력이 출력에 직접 맵핑되는 단일 레이어 신경망은 히든 레이어가 없는 네트워크를 묘사한다. 그런 의미에서, 로지스틱 회귀 또는 SVM을 단일 레이어 신경망의 특별한 경우라고 말할 수 있다. 이 네트워크는 "인공 신경망 (ANN) " 또는 "다중 레이어 퍼셉트론 (MLP)" 이라고도 불린다. 대부분 뉴런 네트워크와 실제 두뇌 사이의 유사성을 좋아하지 않으며, 뉴런을 유닛으로 지칭하는 것을 선호한다 .
- Output layer. 신경망의 다른 레이어들과 달리, 출력 레이어 뉴런은 일반적으로 활성화 함수가 없다. (또는 선형 항등(identity)함수가 있다고  볼 수 있다.). 이것은 최종 출력 레이어가 일반적으로 클래스 스코어(예 : 분류)를 의미하거나, 임의의 실수 값 또는 일종의 실수 값을 가지는 타겟(예 : 회귀) 나타내기 때문이다.  

일반적으로 딥러닝이라 불리는 최신 컨볼루션 네트워크는 약 1억 개의 파라미터를 포함하며, 10-20 여개의 레이어로 구성된다. 그러나 파라미터 공유로 인해 실제 연결 수는 매우 많다. Convolutional Neural Networks 장에서 이에 대한 자세한 내용을 이어가겠다.

##### Example feed-forward computation

활성화 함수와 얽힌 반복 행렬 곱셈 . Neural Networks가 레이어로로 구성되는 주된 이유 중 하나는 이 구조가 행렬 벡터 연산을 사용하여 신경망을 실행하는 것을 매우 간단하고 효율적으로 만들기 때문이다. 위의 다이어그램에서 3-layer 신경망의 예에서 입력은 [3x1] 벡터가 된다. 레이어의 모든 연결 상태는 단일 행렬에 저장 할 수 있다. 예를 들어, 첫 번째 히든 레이어의 웨이트 파라미터 W1은 크기 [4x3]이고 모든 바이어스 b1은 크기 [4x1]인 벡터다. 여기에 모든 단일 뉴런의 행에 대한 웨이트가 W1있으므로 행렬 벡터 내적 np.dot(W1,x)는 해당 레이어의 모든 뉴런에 대한 활성화를 실행간다. 비슷하게, 두 번째 숨겨진 레이어의 연결을 저장하는 [4x4] 매트릭스  W2 와마지막 (출력) 레이어의 [1x4] 매트릭스  W3가 있을 것이다. 이 3 레이어 신경망의 전체 순방향 계산은 활성화 함수를 포함 한 3 개의 행렬 곱셈이다.

```python
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)
```

위의 코드에서  W1,W2,W3,b1,b2,b3은 학습 가능한 네트워크의 파라미터다. 또한 단일 입력 열 벡터 대신 변수 x가 전체 학습 데이터를 나타내며, 모든 샘플들은 효율적으로 병렬로 실행된다. 뉴럴 네트워크의 출력 레이어에는 일반적으로 활성화 함수가 없다 (예 : 분류기에서 클래스 점수(실수 값)를 나타냄).

*완전 연결 레이어(fully-connected layer)의 순방향 패스는 행렬 곱셈에 이은 바이어스 오프셋 및 활성화 함수 적용까지 해당된다.*

##### Representational power

완전 연결 레이어 신경망은 네트워크의 가중치로 파라미터화 함수를 정의한다고 볼 수도 있다. 이때 다음과 같은 질문이 자연스럽게 발생한다. 이 특정함수에 대해 얼마나 잘 표현 할 수 있는가? 특히, 신경망으로 모델링 할 수없는 함수가 있는가?
적어도 하나의 히든 레이어가있는 신경망은 함수에 대한 보편적인 근사 를 표현할 수 있다. 즉, 연속함수 $f(x)$와 $\epsilon > 0$아 았을때, 아래 식을 만족하는 적어도 하나의 비선형성을 포함하는 히든레이어가 있는 신경망 $g(x)$가 존재한다. 
$$\forall x, \mid f(x) - g(x) \mid < \epsilon$$
다시 말하면, 신경망은 연속함수에 대한 근사를 표현한다. (Approximation by Sigmoidal Function 또는 Michael Nielsen의 설명 참조) 
 하나의 히든 레이어로 모든 함수에 대한 근사를 표현할 수 있다면, 왜 더 많은 레이어를 사용하고 더 깊이 들어가야할까? 이에 대한 답은 2 layer 신경망이 수학적으로는 보편적인 근사를 만족하지만 실제로는 별로 쓸모 없는 수준이기 때문이다. 파라미터 벡터 $a, b, c$를 포함하는 함수 $g(x) = \sum_i c_i \mathbb{1}(a_i < x < b_i)$는 분명 일반적인 근사함수로 사용 할 수 있지만, 아무도 이런 함수를 머신러닝에 사용하지 않을 것이다. 뉴럴 네트워크는  실제로 사용하는 데이터의 통계적 특성에 잘 맞는 멋지고 부드러운 함수를 콤팩트하게 표현하며, 최적화 알고리즘 (예 : 그래디언트 디센트)을 사용하여 각 파라미터가 학습되기 때문에 실제로 잘 동작 할 수 있다. 마찬가지로, 더 큰 네트워크 (다중 히든 레이어 포함)가 단일 히든 레이어 네트워크보다 잘 작동한다는 사실은 표현력이 동일하다는 수학적 식에도 불구하고 실험적으로 관찰된다. 

 실제로 3 레이어 신경망이 2 레이어 신경망 보다 성능이 좋은 경우는 많지만, 더 깊은 (4,5,6 층)은 거의 성능 향상에 도움이 되지 않는다. 이는 깊이가 인식 시스템의 성능에서 매우 중요한 구성 요소인 것으로 밝혀진 Convolutional Networks와 완전히 대조된다. 이러한 사실에 대한 한 가지 논거는 이미지가 계층 구조를 포함한다는 것이다 (예 : 얼굴의 구성요소에는 눈이 있으며, 눈은 선들로 구성됨). 따라서 이미지 처리에서는 여러 계층을 두는 것이 성능향상에 유리하다는 사실에 직관적으로 받아들여진다. 

이에 대한 자세한 내용은 최신 논문들에서 다뤄지고 있다. 이 주제에 대한 관심이 있다면 아래를 참조하기 바란다. 

- [Deep Learning book](http://www.deeplearningbook.org/) in press by Bengio, Goodfellow, Courville, in particular Chapter 6.4.
- [Do Deep Nets Really Need to be Deep?](http://arxiv.org/abs/1312.6184)
- [FitNets: Hints for Thin Deep Nets](http://arxiv.org/abs/1412.6550)

##### Setting number of layers and their sizes

실제 문제를 모델링 할때 신경망에 사용할 아키텍처를 어떻게 결정할까? 히든 레이어를 사용할 필요가 없을까? 하나의 히든 레이어? 두 개의 히든 레이어? 각 레이어는 얼마나 커야할까? 첫째, 뉴럴 네트워크에서 레이어의 크기와 수가 늘어 날 수록 네트워크 용량이 증가한다는 점에 유의하라. 즉, 뉴런이 많을 수록 다양한 함수를 표현하기 위해 상호작용도 증가하기 때문에 표현 가능한 함수의 양이 늘어 난다. 예를 들어, 2차원에서 2진 분류 문제가 있다고 가정하자. 아래와 같은 특정 크기를 가지는 하나의 히든 레이어를 포함하는 학습된 세 개의 개별 신경망이 있을 때,  각 신경망은  다음과 같은 분류기를 얻는다.

![](http://cs231n.github.io/assets/nn1/layer_sizes.jpeg "layer_sizes" "width:600px;float:center;padding-left:10px;")

큰 신경망은 더 복잡한 함수를 나타낼 수 있다. 데이터는 클래스별로 색이 있는 점으로 표시되며, 훈련 된 신경망에 의한 분류 영역도 표시된다.  [ConvNetsJS 데모](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)에서 이 예제들을 가지고 놀 수 있다 .

위의 다이어그램에서 뉴런이 더 많은 뉴런 네트워크는 더 복잡한 함수를 표현할 수 있음을 알 수 있다. 그러나 이것은 축복(더 복잡한 데이터에 대한 분류를 학습 할 수 있기 때문에)이자, 저주(학습 데이터에 오버피팅 되기 쉽기 때문에)이다. **오버피팅(Overfitting)**은 큰 모델이 (예상되는) 결과 대신 데이터의 노이즈에 맞춰 동작 할 때 발생된다. 예를 들어, 20 개의 히든 뉴런을 가진 모델은 모든 학습 데이터에 적합하게 동작하지만 공간을 여러 개의 녹색, 적색 영역으로 분리한다. 3개의 히든 뉴런을 가진 모델은 데이터를 넓은 스트로크로 분류한다. 여기서는 데이터를 두 영역으로 모델링하고 녹색 클러스터 내부의 적색 점을 특이값(노이즈)으로 해석한다. 실제로 이것은 테스트 세트에 대한 더 나은 일반화를 이끌어 낸다. 

위의 논의를 토대로, 데이터가 **오버피팅**이 발생되지 않을 만큼 간단하다면, 작은 규모의 뉴럴 네트워크가 선호 될 수 있다. 그러나 이것이 잘못된 방법이다. - 신경망에서 오버피팅을 방지하기 위해 선호되는 다른 방법(예 : L2 정규화, 드롭 아웃, 입력 노이즈)이 있다. 실제로는 뉴런 수 대신 오버피팅을 제어하는 ​​데 이 방법을 사용하는 것이 좋다.

이에 대한 자세한 이유는 더 작은 네트워크가 Gradient Descent와 같은 지역적 방법으로는 학습 시키기 더 어렵다는 것이다. 이 경우 로스 함수의 로컬 최소점이 비교적 적지만, 이 로컬 최소 점으로 수렴하기 쉽고 그럴 경우 로스 값이 큰 안 좋은 결과가 나온다. 반대로, 더 큰 뉴럴 네트워크는 상당히 더 많은 로컬 최소점을 포함하지만, 이러한 최소점은 실제 로스의 측면에서 훨씬 더 좋은 결과(낮은 로스값)을 보인다. 뉴럴 네트워크는 non-convex이기 때문에 이러한 특성을 수학적으로 연구하기는 어렵지만, 최근의 논문([다층 네트워크의 손실 표면 (Loss Surfaces of Multilayer Networks](http://arxiv.org/abs/1412.0233))에서 위 특성을 다뤘다. 실제로, 작은 네트워크를 학습 시킬 때, 최종 로스 값이 상당한 차이를 보일 수 있다. - 경우에 따라 운이 좋을 때는 좋은 값으로 수렴하지만 어떤 경우에는 안 좋은 최소 점으로 수렴 할 수 있다.  큰 네트워크의 경우 다양한 최소 점으로 수렴 될 수 있지만, 최종 로스 값의 편차는 훨씬 적다. 다른 말로하면, 이 경우 모든 솔루션은 동등하게 우수하며, 무작위 초기화에 대한 운에 덜 의존한다. 

앞에서도 말했지만, **정규화(regularization)**는 신경망의 오버피팅을 제어하는데 선호된다. 다음은 세 가지 설정에 따른 결과를 보여준다. 

![](http://cs231n.github.io/assets/nn1/reg_strengths.jpeg "reg_strengths" "width:600px;float:center;padding-left:10px;")

정규화의 효과 : 위의 각 신경망에는 20 개의 숨겨진 뉴런이 있지만 정규화 값을 키우면 최종 분할 영역이 더 부드럽게 된다.  [ConvNetsJS 데모](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html) 에서 이 예제들을 가지고 놀 수 있다.

시사점은 오버피팅이 두려워서 작은 네트워크를 사용하지 말아야 한다는 것이다. 대신 계산량이 허용하는 만큼 큰 신경망을 사용한 다음, 다른 정규화 기법을 통해 오버피팅을 제어해야한다.

##### Summary

요약하자면,

- 매우 단순한 생물학적 뉴런 모델을 소개했다. 
- 실제 사용되는 가장 일반적인 활성화 함수인 ReLU 및 몇 가지 유형의 활성화 함수(activation functions)에 대해 논의했다.
- 뉴런들이 서로 연결된 뉴런 네트워크와, 인접한 레이어의 뉴런과 완전히 페어 와이즈로 연결되어 있지만, 레이어 내의 뉴런들은 서로 연결되지 않은 완전 연결 레이어(Fully-Connected layers)를 소개했다. 
- 이러한 레이어 아키텍처는 활성화 함수을 적용한 매트릭스 곱을 기반으로 신경망을 매우 효율적으로 동작시킬 수 있음을 확인했다.
- 신경망이 보편적인 함수 근사라는 것을 알았지만, 이 속성은 일반적인 사용과 관련이 거의 없다는 사실에 대해서도 논의했다. 실제로는 사용되는 함수의 기술적인 형태에 대한 "올바른"가정을 통해 사용된다.
- 규모가 큰 네트워크가 소규모 네트워크 보다 항상 잘 작동 할 것이라는 사실에 대해 논의했으며, 더 큰 모델 용량은 더 강한 정규화 (예 : 더 높은 웨이트 감소)로 적절히 해결해야 한다. 이후 섹션에서 정규화(특히 dropout)의 더 많은 형태를 보게 될 것이다.

##### Additional References

- [deeplearning.net tutorial with Theano](http://www.deeplearning.net/tutorial/mlp.html)
- [ConvNetJS demos](http://cs.stanford.edu/people/karpathy/convnetjs/) for intuitions
- [Michael Nielsen’s tutorials](http://neuralnetworksanddeeplearning.com/chap1.html)
